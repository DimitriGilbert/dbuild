1:"$Sreact.fragment"
2:I[22304,["239","static/chunks/239-c47e640cea1e8478.js","802","static/chunks/802-cb271f7d125031e5.js","177","static/chunks/app/layout-8f6931abe4e72e4f.js"],"ThemeProvider"]
3:I[71000,["239","static/chunks/239-c47e640cea1e8478.js","802","static/chunks/802-cb271f7d125031e5.js","177","static/chunks/app/layout-8f6931abe4e72e4f.js"],"FloatingNavigation"]
4:I[13842,[],""]
5:I[86880,[],""]
7:I[25803,[],"OutletBoundary"]
9:I[25803,[],"MetadataBoundary"]
b:I[25803,[],"ViewportBoundary"]
d:I[16773,[],""]
:HL["/_next/static/media/e4af272ccee01ff0-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/_next/static/css/50152c3edddaf6a5.css","style"]
0:{"P":null,"b":"xu25PFW4HmsoruEgi_bR5","p":"","c":["","blog","tags","ai",""],"i":false,"f":[[["",{"children":["blog",{"children":["tags",{"children":[["tag","ai","d"],{"children":["__PAGE__",{}]}]}]}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/50152c3edddaf6a5.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","suppressHydrationWarning":true,"children":[["$","head",null,{"children":["$","script",null,{"src":"https://cdn.counter.dev/script.js","data-id":"154c6878-7558-4eff-90f9-bd4904015df1","data-utcoffset":"1","async":true}]}],["$","body",null,{"className":"__className_f367f3","children":["$","$L2",null,{"attribute":"class","defaultTheme":"dark","enableSystem":true,"disableTransitionOnChange":true,"children":[["$","$L3",null,{}],["$","main",null,{"children":["$","$L4",null,{"parallelRouterKey":"children","segmentPath":["children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[],[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]]],"forbidden":"$undefined","unauthorized":"$undefined"}]}]]}]}]]}]]}],{"children":["blog",["$","$1","c",{"children":[null,["$","$L4",null,{"parallelRouterKey":"children","segmentPath":["children","blog","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":["tags",["$","$1","c",{"children":[null,["$","$L4",null,{"parallelRouterKey":"children","segmentPath":["children","blog","children","tags","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["tag","ai","d"],["$","$1","c",{"children":[null,["$","$L4",null,{"parallelRouterKey":"children","segmentPath":["children","blog","children","tags","children","$0:f:0:1:2:children:2:children:2:children:0","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":["__PAGE__",["$","$1","c",{"children":["$L6",null,["$","$L7",null,{"children":"$L8"}]]}],{},null,false]},null,false]},null,false]},null,false]},null,false],["$","$1","h",{"children":[null,["$","$1","514esMdqkWscBrBUmv1Dy",{"children":[["$","$L9",null,{"children":"$La"}],["$","$Lb",null,{"children":"$Lc"}],["$","meta",null,{"name":"next-size-adjust","content":""}]]}]]}],false]],"m":"$undefined","G":["$d","$undefined"],"s":false,"S":true}
e:I[73104,["239","static/chunks/239-c47e640cea1e8478.js","802","static/chunks/802-cb271f7d125031e5.js","792","static/chunks/app/blog/tags/%5Btag%5D/page-af6646a5e411dccb.js"],"TagDetailContent"]
f:T3d6f,
I wont reintroduce the concept of Ralph loop, if you have been anywhere near AI in the first half of January 2026, you have heard of it ! If not, you should go and watch [Theo's video](https://www.youtube.com/watch?v=Yr9O6KFwbW4) (or someone else's but this is a good !)

You probably know which comics character Ralph is refering to, but Eric... who could it possibly be ?

If you thought short, fat and angry, you'd be right ! I'm of course talking about Eric Cartman from South Park !

While Ralph is naive and innocent, Eric is calculating and manipulative. Not something I'd look for in a Human, but for managing a bunch of AIs ? HELL YEAH !

## The Eric Loop

An idea, even if good, always need some work and some back and forth to be implemented. So we first expose the idea to the AI to get an requirement document (PRD) and we work on it !

How ? you'll need to read first ! and straight away update parts you don't agree with or things you need to be more thorough !

Once that is done, ask AI models to ask You questions about the PRD and give all those, the PRD and your replies to another AI for update

Rince, Repeat, until you are satisfied with the document.

You can even get multiple model go at it to then compare and merge the outputs, using AI, of course :D

This is where we start diverging from the basic Ralph loop. instead of just passing that to the AI and hoping tasks will be implemented properly we get an AI to pre split and formalise the tasks and sub tasks into a list !

The rift grows further as we split the task implementation in several steps with:
* a planning phase that analyzes the requirements for the tasks, existing code, and conduct the needed research
* an execution phase to do the actual work,
* an optional automated verification phase (check types, build, tests, ...) with automated feedback to the execution phase in case of errors,
* an result review phase, that can send back to the execution phase for further refinement.

Once all that is done ? the Eric loop goes on for the next phase of the plan, just like Ralph !

## But, Why... ?

Separation of concerns !

Yes models have gotten smarter, but if you played around with sub agents and skills, you know that prompting is not dead (yet...)

Having a neat separation like that helps you have better prompts for each phase of the task execution.

As a side effect, it mmeans you can control which model is used for which phase maybe you dont need Opus the whole way if it made a banger plan !

## What would it look like ?

We are going to create a small project from an idea I have, you can do it to if you like !

It's called Tiny-till, a small app to have a simple till for itinerant merchants.

But we are going to need a tool to help us ... 

### Task-o-matic

Yep, I already kind of made said tool :D ! [task-o-matic](https://task-o-matic.dev)

I'll write more about it in a future post ! but you already have plenty of content to go through on the site itself.

I made sure to generate a lot of docs and a couple tutorials so you can already give it a try if you'd like !

But in a nutshell, it helps you create PRDs, refine them , split into tasks and subtasks and finally wiring all that to your favorite AI harness !

### Let's get this rolling

For the AI to operate properly, we need a specific stack, that way, we limit hallucination !

#### Initialisation and bootstrapping

Task-o-matic uses (https://better-t-stack.dev) under the hood to bootstrap the stack so let's do that !

```bash
npx task-o-matic@latest init init --project-name tiny-till \
  --frontend tanstack-router \
  --backend none \
  --package-manager bun && cd tiny-till
```

Boom, a monorepo for Tiny-till ! Batteries included : tailwind, shadcn, build script and all :D

Why `init init` ? you might have missed the whole using AI to code whole projects, sometimes... it is not aesthetically pleasing, and you should learn not to care to much (is my take on that... or lazyness...)

You will need a .env file to configure the AI stack.

```ini
AI_PROVIDER=openrouter

AI_MODEL=nvidia/nemotron-3-nano-30b-a3b:free

OPENROUTER_API_KEY=sk_123456azertPOIUY7890i58008
```

#### Document the requirements of your project

Now, we tell the AI what we want to do, but why would we limit ourselves to 1 ?

```bash
npx task-o-matic@latest prd create \
  --ai-reasoning 4096 \
  --stream \
  --ai "xiaomi/mimo-v2-flash:free;reasoning=4096" \
  --ai "mistralai/devstral-2512:free:free" \
  --ai "nvidia/nemotron-3-nano-30b-a3b:free;reasoning=4096" \
  --ai "minimax/minimax-m2.1;reasoning=4096" \
  --ai "z-ai/glm-4.7;reasoning=4096" \
  --ai "google/gemini-3-flash-preview;reasoning=4096" \
  --combine-ai "z-ai/glm-4.7;reasoning=4096" \
  "i would like to build tiny-till,
  it will be a small web app, local first (for sparse internet connection) that serve as a very simple till for on the go seller, farmers markets stall, etc...
  the idea came from me watching my baker who does delivery tour twice a week. she is always fighting here calculator to make the tally and she gromle about it a lot
  the idea wwould be only a few screens : 
  - product management (add/remove product, name+price (img optional) ) : a list with action buttons, + or edit happen inline (+ at the top)
  - a tally page : grid of product for fast and easy access (row of 2 or 3 for mobile, 4 to 6 on tablet, normal/compact variant, control by setting)
    - clicking on a product adds it to the count
    - number of this product added appear in the card, click on it trigger an input field to enter value with a remove button
  - a setting page : theme, display, import/export catalog
  No saving, it is purely done too calculate the price and is not meant for accounting purposes
  i am using tanstack-router and the project is already set up with tailwind css typescript and shadcnUI"
```

Am I using free endpoints for "real" work ? Yes, yes I am ! Does it distort my views on how much AI work should cost ? ... Why would it ? No, no, you just being a killjoy right now !

#### Petrol is not the only thing that needs refining !

A few minutes later... I got myself this (`cat .task-o-matic/prd/prd-master.md`)[/assets/blackhole/from-ralph-to-eric/prd-master.md] it doesn't really need more questions, but let's see what Claude has to say, shall we ?

```bash
npx task-o-matic@latest prd question  --stream \
  --file .task-o-matic/prd/prd-master.md \
  --ai-model anthropic/claude-4.5-sonnet --ai-reasoning 1500
```

Well, let's answer those... some are pertinant ! I guess that's why we bring out the big guns !

Let's reply !

```bash
npx task-o-matic@latest prd refine  --stream\
  --file .task-o-matic/prd/prd-master.md \
  --ai-model anthropic/claude-4.5-sonnet --ai-reasoning 1500 \
  --output .task-o-matic/prd/prd-master-refined.md \
  --questions prd-questions.json
```

Questions are asked interactivelly ;)

```markdown
"The PRD mentions using Zustand with persist middleware for the Catalog, but also specifies IndexedDB as the storage layer. Should Zustand's persist middleware be configured to use IndexedDB directly, or will there be a custom sync layer between Zustand and IndexedDB? This affects how image blobs are handled since Zustand persist middleware typically works best with serializable data."

yes, configure Zustand's persist middleware to use IndexedDB directly.
```

```markdown
"For the 'Tap Badge to manually enter quantity' feature, what should happen if a user enters invalid input (negative numbers, decimals, non-numeric characters, or zero)? Should zero be treated as 'remove from cart' or should it maintain the item with 0 quantity until explicitly removed?",

zero should be treated as 'remove from cart'.
```

```markdown
"The PRD states the tally is 'transient' and held in memory, yet also mentions using sessionStorage. Given that sessionStorage persists across page refreshes within the same tab, this conflicts with the 'privacy and simplicity' goal of resetting on refresh. Should the tally truly reset on refresh (pure memory), or should it survive accidental refreshes during a transaction (sessionStorage)?",

no persistance, this is not for accounting and should only be in memory, when a new tally starts, the previous one does not exist anymore.
```

```markdown
"With TanStack Router's file-based routing, how should the navigation structure be organized? Specifically, should the Tally Page be the root route ('/'), and if users are on the Settings or Catalog management page when they refresh, should they be automatically redirected back to the Tally Page to prevent data loss confusion?",

yes, / is the tally page, yes redirect to tally page.
```

```markdown
"The image upload feature is marked as 'optional' for products, but the PRD recommends 'no images for MVP' in the risk mitigation. Should image upload functionality be completely excluded from MVP (Phase 1-5), or should the data model and UI be built to accommodate images even if the upload feature is disabled/hidden initially?",

no, i want images, just inforce a strict size limitation, 128*128 px, this is not meant to be a full image gallery, just a thumbnail for display, future improvement maybe to try and resize/optimise bigger image directly on device...but not in MVP !
```

```markdown
"For the Import/Export catalog feature, what should happen when importing a JSON file that contains product IDs that conflict with existing products? Should it merge (overwrite matching IDs), append (create duplicates), or prompt the user to choose? Also, should the import validate the JSON schema and handle corrupt/invalid files gracefully?",

overwrite and yes, must validate.
```

```markdown
"The 'Compact vs Normal' view toggle affects card size, but how should this interact with the responsive column count? For example, if a tablet user sets 6 columns in Normal view, should switching to Compact view automatically increase columns (e.g., to 8) to utilize the space savings, or should column count remain fixed per the user's explicit setting?",

yes, auto increase, 'tis why we want compact !
```

```markdown
"Given the Turborepo setup mentioned in the stack, is this application intended to be part of a monorepo with potential future packages (e.g., a companion admin dashboard, analytics service)? If so, should the data models and storage utilities be architected in a shared package from the start, or is this truly a standalone application for now?"

no, single package for now, we'll see later for the rest. let's keep it simple for now.
```

Well, this little stunt just costed me a nice 15 cts, more or les 10x more than the first PRD generation...

I don't know what is the most amazing... that 

* 1 call costs more than the 10 previous, 
* I can complain about a 2 days worth of work document cost 15 cents for only a few seconds of work !
* a document worth 2 of my days of work costs 15 cents to produce...in only a few seconds...

I haven't decided yet...anywooooooo, time to move on and not get siderated !

#### Touching up and go

When i said seconds, i lied, I spent 5 minutes editing the file. to fix lib versions, marked install and config work as done, and, finally, set expectation for the design.

the last thing we have to do, is ask an AI to split into main tasks.

once again, i wont only do 1, but this time, I'll ask Claude to be the final task creator, from others input !

```bash
npx task-o-matic@latest prd parse  --ai-reasoning 4096 --stream \
  --ai "xiaomi/mimo-v2-flash:free;reasoning=4096"\
  --ai "nvidia/nemotron-3-nano-30b-a3b:free;reasoning=4096" \
  --ai "z-ai/glm-4.7;reasoning=4096" \
  --combine-ai "anthropic/claude-4.5-sonnet;reasoning=4096"
```

And there goes another 8.2 cents burnt for the token Gods, but in return, we get a detailed breakdown of the tasks required to complete the project !

We still have to split those tasks though... To make them more palatable for the current crop of AI models ! 

You should really consider reviewing the tasks in detail before spliting, not vibe planning like a muppet ! (guess what I did ^^)

```bash
npx task-o-matic@latest tasks split  --all --stream \
  --reasoning 4096 \
  --ai "xiaomi/mimo-v2-flash:free;reasoning=4096"\
  --ai "nvidia/nemotron-3-nano-30b-a3b:free;reasoning=4096" \
  --ai "z-ai/glm-4.7;reasoning=4096" \
  --combine-ai "anthropic/claude-4.5-sonnet;reasoning=4096"
```

I think Claude is not necessary here, but I had me $3 of openrouter credit i have to burn before the end of the month soooooo... big guns it is !

Burnt a tenth of that ! but now... we are actually ready ! Almost...

#### AGENTS.md

Or CLAUDE, or GEMINI, depends on your harness of choice, For me, it is opencode i'am going to run the /init prompt in opencode !

```bash
opencode run "/init"
```

it is not mandatory, but it is a good way to prevent dumb mistake and set some proper behavior regarding a few things, for me, i'll add the following

* Not to run the dev server
* not start background/long running processes
* not to use `:any`, `as any` or `any` as type, I want a typesafe codebase
* before claiming success, `bun run check-types` and `bun run build` must both succeed
* to fix LSP error when they happen
* use shadcn component has much as possible
* stay DRY and prefer creating custom components and utilities than repeat code
* use adapted skills when possible (frontend-design skill is mandatory when working on the frontend)
* that it is a static app project that will be hosted on github pages
* to use Context7 MCP in case more documentation or up to date documentation is needed (as the knowledge is certainly behind because of cutoff training date)

```bash
opencode run "Add the following to the AGENTS.md, remove any conflicting instructions you created, these take precedence : 
* Not to run the dev server
* not start background/long running processes
* not to use `:any`, `as any` or `any` as type, I want a typesafe codebase
* before claiming success, `bun run check-types` and `bun run build` must both succeed
* to fix LSP error when they happen
* use shadcn component has much as possible
* stay DRY and prefer creating custom components and utilities than repeat code
* use adapted skills when possible (frontend-design skill is mandatory when working on the frontend)
* that it is a static app project that will be hosted on github pages
* to use Context7 MCP in case more documentation or up to date documentation is needed (as the knowledge is certainly behind because of cutoff training date)" -c
```

I might add more as the agent works if i see reccuring errors and dumb things happening

#### Time to let Eric loose...

Well, guys, stash your parents away cause we are going to let Eric go at it !

Because I am El CheapoDev DelBrokeCasa, i am going to use my GLM coding plan to code, that way, i wont break the bank and it is OK enough for something like that i think !

If you'd rather use claude code, use the `--tool claude` option. (or codex/gemini/kilo) but opencode is nicer as it stream the content out (in part at least) so you see what is happening !

```bash
npx task-o-matic@latest tasks execute-loop --include-prd \
  --plan --plan-model zai-coding-plan/glm-4.7 \
  --review --review-model zai-coding-plan/glm-4.7 \
  --model zai-coding-plan/glm-4.7 \
  --validate "npm run check-types && npm run build"
```

And now, you wait, coffee, snacks, more coffee, diner and breakfast probably, it gonna take a while ^^

## What do you think ? Where do we go from here ?

So that's the end (of this article) ! What do you think ? did you try a similar technique ? What's the plan next, cause...

If you are interested, the repo can be found on [github](https://github.com/DimitriGilbert/tiny-till)
10:T4479,
Je vais pas re-introduire le concept de Ralph loop, si vous avez √©t√© a c√¥t√© de l'IA dans la premi√®re moiti√© de janvier 2026, vous en avez entendu parler ! Si non, vous devriez aller voir la vid√©o de Theo (ou quelqu'un d'autre mais celle-la est bonne !)

Vous savez probablement quel personnage de comics Ralph r√©f√©rence, mais Eric... ca pourrait √™tre qui ?

Si vous avez pens√© petit, gros et en col√®re, vous avez raison ! Je parle bien s√ªr d'Eric Cartman de South Park !

Alors que Ralph est na√Øf et innocent, Eric est calculateur et manipulateur. Pas quelque chose que je chercherais chez un humain, mais pour g√©rer un tas d'IA ? OUAIS DEFINITIVEMENT !

## La boucle Eric

Une id√©e, m√™me si elle est bonne, a toujours besoin de travail et d'allers-retours pour √™tre impl√©ment√©e. Donc on expose d'abord l'id√©e √† l'IA pour avoir un Product Requirements Document (PRD) et on travaille dessus !

Comment ca, lire ? Et bhe oui, il faut d'abord lire ! et tout de suite mettre √† jour les parties avec lesquelles vous n'√™tes pas d'accord ou les choses qui doivent √™tre plus approfondies !

Une fois que c'est fait, demandez aux mod√®les d'IA de vous poser des questions sur le PRD et donnez tout √ßa, le PRD et vos r√©ponses, √† une autre IA pour la mise √† jour.

Rincez, R√©p√©tez, jusqu'√† ce que vous soyez satisfait du document.

Vous pouvez m√™me faire traiter le PRD par plusieurs mod√®les pour ensuite comparer et fusionner les r√©sultats, en utilisant l'IA, bien s√ªr :D

C'est l√† qu'on commence √† diverger de la boucle Ralph de base. Au lieu de juste passer √ßa √† l'IA et esp√©rer que les t√¢ches seront bien impl√©ment√©es, on fait pr√©-d√©couper et formaliser les t√¢ches et sous-t√¢ches en une liste par une IA !

L'√©cart grandit encore plus car on s√©pare l'impl√©mentation des t√¢ches en plusieurs √©tapes avec :
* une phase de planification qui analyse les requis pour les t√¢ches, le code existant, et fait les recherches n√©cessaires
* une phase d'ex√©cution pour faire le travail r√©el
* une phase optionnelle de v√©rification automatis√©e (check types, build, tests, ...) avec feedback automatique vers la phase d'ex√©cution en cas d'erreurs
* une phase de review des r√©sultats, qui peut renvoyer vers la phase d'ex√©cution pour plus de raffinement.

Une fois tout √ßa fait ? la boucle Eric continue pour la phase suivante du plan, tout comme Ralph !

## Mais, Pourquoi... ?

La s√©paration des pr√©occupations !

Oui les mod√®les sont devenus plus intelligents, mais si vous avez jou√© avec des sub-agents et des skills, vous savez que le prompting n'est pas mort (pour le moment...)

Avoir une s√©paration propre comme √ßa aide √† avoir de meilleurs prompts pour chaque phase de l'ex√©cution des t√¢ches.

Comme effet secondaire, √ßa signifie que vous pouvez contr√¥ler quel mod√®le est utilis√© pour quelle phase, peut-√™tre que vous n'avez pas besoin d'Opus tout le long si √ßa a fait un super plan !

## A quoi √ßa ressemblerait ?

On va cr√©er un petit projet √† partir d'une id√©e que j'ai, vous pouvez le faire aussi si vous voulez !

Il s'appelle Tiny-till, une petite app pour avoir une caisse simple pour marchands ambulants.

Mais on va avoir besoin d'un outil pour nous aider ...

### Task-o-matic

Ouais, j'ai d√©j√† en gros fait l'outil en question :D ! [task-o-matic](https://task-o-matic.dev)

J'√©crirai plus √† ce sujet dans un futur post ! mais vous avez d√©j√† pas mal de contenu √† parcourir sur le site lui-m√™me.

J'ai fait gaffe √† g√©n√©rer plein de docs et quelques tutoriels pour que vous puissiez d√©j√† essayer si vous voulez !

Mais en gros, √ßa aide √† cr√©er des PRDs, les raffiner, d√©couper en t√¢ches et sous-t√¢ches et finalement connecter tout √ßa √† votre outil de code assit√© d'IA pr√©f√©r√© !

### Allons-y

Pour que l'IA op√®re proprement, on a besoin d'une stack sp√©cifique, comme √ßa, on limite les hallucinations !

#### Initialisation et bootstrap

Task-o-matic utilise (https://better-t-stack.dev) en interne pour bootstrap la stack donc faisons √ßa !

```bash
npx task-o-matic@latest init init --project-name tiny-till \
  --frontend tanstack-router \
  --backend none \
  --package-manager bun && cd tiny-till
```

Boum, un monorepo pour Tiny-till ! Batteries incluses : tailwind, shadcn, script de build et tout :D

Pourquoi `init init` ? vous avez peut-√™tre rat√© toute l'histoire de l'IA qui code des projets entiers, parfois... c'est pas esth√©tique, et vous devriez apprendre √† pas trop y faire attention (c'est mon avis la-dessus... ou de la flemme...)

Vous aurez besoin d'un fichier .env pour configurer la stack IA.

```ini
AI_PROVIDER=openrouter

AI_MODEL=nvidia/nemotron-3-nano-30b-a3b:free

OPENROUTER_API_KEY=sk_123456azertPOIUY7890i58008
```

#### Documenter les requis de votre projet

Maintenant, on dit √† l'IA ce qu'on veut faire, mais pourquoi se limiter √† 1 ?

```bash
npx task-o-matic@latest prd create \
  --ai-reasoning 4096 \
  --stream \
  --ai "xiaomi/mimo-v2-flash:free;reasoning=4096" \
  --ai "mistralai/devstral-2512:free:free" \
  --ai "nvidia/nemotron-3-nano-30b-a3b:free;reasoning=4096" \
  --ai "minimax/minimax-m2.1;reasoning=4096" \
  --ai "z-ai/glm-4.7;reasoning=4096" \
  --ai "google/gemini-3-flash-preview;reasoning=4096" \
  --combine-ai "z-ai/glm-4.7;reasoning=4096" \
  "i would like to build tiny-till,
  it will be a small web app, local first (for sparse internet connection) that serve as a very simple till for on the go seller, farmers markets stall, etc...
  the idea came from me watching my baker who does delivery tour twice a week. she is always fighting here calculator to make the tally and she gromle about it a lot
  the idea wwould be only a few screens :
  - product management (add/remove product, name+price (img optional) ) : a list with action buttons, + or edit happen inline (+ at the top)
  - a tally page : grid of product for fast and easy access (row of 2 or 3 for mobile, 4 to 6 on tablet, normal/compact variant, control by setting)
    - clicking on a product adds it to the count
    - number of this product added appear in the card, click on it trigger an input field to enter value with a remove button
  - a setting page : theme, display, import/export catalog
  No saving, it is purely done too calculate the price and is not meant for accounting purposes
  i am using tanstack-router and the project is already set up with tailwind css typescript and shadcnUI"
```

J'utilise des endpoints gratuits pour du "vrai" travail ? Oui, oui, je fais ! Est-ce que √ßa fausse ma vision de combien le travail IA devrait co√ªter ? ... pourquoi √ßa ferait √ßa ? Non, non, vous √™tes juste en train de g√¢cher la f√™te l√† !

#### L'essence n'est pas la seule chose qui doit √™tre raffin√©e !

Quelques minutes plus tard... je me suis r√©cup√©r√© √ßa (`cat .task-o-matic/prd/prd-master.md`)[/assets/blackhole/from-ralph-to-eric/prd-master.md] √ßa a pas vraiment besoin de plus de questions, mais voyons ce que Claude a a dire, d'accord ?

```bash
npx task-o-matic@latest prd question  --stream \
  --file .task-o-matic/prd/prd-master.md \
  --ai-model anthropic/claude-4.5-sonnet --ai-reasoning 1500
```

Bon, r√©pondons √† √ßa... certaines sont pertinentes ! je suppose que c'est pour √ßa qu'on sort les gros canons !

R√©pondons !

```bash
npx task-o-matic@latest prd refine  --stream\
  --file .task-o-matic/prd/prd-master.md \
  --ai-model anthropic/claude-4.5-sonnet --ai-reasoning 1500 \
  --output .task-o-matic/prd/prd-master-refined.md \
  --questions prd-questions.json
```

Les questions sont pos√©es interactivement ;)

```markdown
"The PRD mentions using Zustand with persist middleware for the Catalog, but also specifies IndexedDB as the storage layer. Should Zustand's persist middleware be configured to use IndexedDB directly, or will there be a custom sync layer between Zustand and IndexedDB? This affects how image blobs are handled since Zustand persist middleware typically works best with serializable data."

oui, configurer le middleware persist de Zustand pour utiliser IndexedDB directement.
```

```markdown
"For the 'Tap Badge to manually enter quantity' feature, what should happen if a user enters invalid input (negative numbers, decimals, non-numeric characters, or zero)? Should zero be treated as 'remove from cart' or should it maintain the item with 0 quantity until explicitly removed?",

z√©ro doit √™tre trait√© comme 'retirer du panier'.
```

```markdown
"The PRD states the tally is 'transient' and held in memory, yet also mentions using sessionStorage. Given that sessionStorage persists across page refreshes within the same tab, this conflicts with the 'privacy and simplicity' goal of resetting on refresh. Should the tally truly reset on refresh (pure memory), or should it survive accidental refreshes during a transaction (sessionStorage)?",

pas de persistance, c'est pas pour la comptabilit√© et √ßa doit √™tre uniquement en m√©moire, quand un nouveau tally commence, le pr√©c√©dent n'existe plus.
```

```markdown
"With TanStack Router's file-based routing, how should the navigation structure be organized? Specifically, should the Tally Page be the root route ('/'), and if users are on the Settings or Catalog management page when they refresh, should they be automatically redirected back to the Tally Page to prevent data loss confusion?",

oui, / est la page tally, oui rediriger vers la page tally.
```

```markdown
"The image upload feature is marked as 'optional' for products, but the PRD recommends 'no images for MVP' in the risk mitigation. Should image upload functionality be completely excluded from MVP (Phase 1-5), or should the data model and UI be built to accommodate images even if the upload feature is disabled/hidden initially?",

non, je veux les images, imposez juste une limitation de taille stricte, 128*128 px, c'est pas fait pour √™tre une galerie d'images compl√®te, juste une miniature pour l'affichage, am√©lioration future peut-√™tre pour essayer de redimensionner/optimiser des images plus grosses directement sur l'appareil...mais pas en MVP !
```

```markdown
"For the Import/Export catalog feature, what should happen when importing a JSON file that contains product IDs that conflict with existing products? Should it merge (overwrite matching IDs), append (create duplicates), or prompt the user to choose? Also, should the import validate the JSON schema and handle corrupt/invalid files gracefully?",

√©craser et oui, doit valider.
```

```markdown
"The 'Compact vs Normal' view toggle affects card size, but how should this interact with the responsive column count? For example, if a tablet user sets 6 columns in Normal view, should switching to Compact view automatically increase columns (e.g., to 8) to utilize the space savings, or should column count remain fixed per the user's explicit setting?",

oui, augmentation auto, c'est pour √ßa qu'on veut compact !
```

```markdown
"Given the Turborepo setup mentioned in the stack, is this application intended to be part of a monorepo with potential future packages (e.g., a companion admin dashboard, analytics service)? If so, should the data models and storage utilities be architected in a shared package from the start, or is this truly a standalone application for now?",

non, package unique pour l'instant, on verra apr√®s pour le reste. gardons √ßa simple pour l'instant.
```

Bon, cette petite astuce vient de me co√ªter un joli 15 cts, plus ou moins 10x plus que la premi√®re g√©n√©ration de PRD...

Je sais pas ce qui est le plus incroyable... que

* 1 appel co√ªte plus que les 10 pr√©c√©dents,
* je puisse me plaindre qu'un document de 2 jours de travail co√ªte 15 cents pour seulement quelques secondes de travail !
* un document qui vaut 2 de mes jours de travail co√ªte 15 cents √† produire...en seulement quelques secondes...

J'ai pas encore d√©cid√©...bon bref, passons √† la suite et restons pas bloqu√©s !

#### Les petites retouches et c'est parti

Quand j'ai dit secondes, j'ai menti, j'ai pass√© 5 minutes √† √©diter le fichier. pour fixer les versions de lib, marquer le travail d'install et de config comme fait, et, enfin, d√©finir les attentes pour le design.

la derni√®re chose qu'on a a faire, c'est demander √† une IA de d√©couper en t√¢ches principales.

encore une fois, je vais pas en faire qu'une seule, mais cette fois, je vais demander √† Claude d'√™tre le cr√©ateur de t√¢ches final, √† partir des entr√©es des autres !

```bash
npx task-o-matic@latest prd parse  --ai-reasoning 4096 --stream \
  --ai "xiaomi/mimo-v2-flash:free;reasoning=4096"\
  --ai "nvidia/nemotron-3-nano-30b-a3b:free;reasoning=4096" \
  --ai "z-ai/glm-4.7;reasoning=4096" \
  --combine-ai "anthropic/claude-4.5-sonnet;reasoning=4096"
```

Et voila encore 8.2 cents br√ªl√©s en offrandes aux dieux du token, mais en retour, on a un d√©coupage d√©taill√© des t√¢ches requises pour finir le projet !

On a encore √† d√©couper ces t√¢ches par contre... Pour les rendre plus dig√©rables, m√™me pour les meilleurs mod√®les IA √† l'heure actuelle !

Ca serait une bonne id√©e de revoir les t√¢ches en d√©tail avant de les d√©couper, pas faire du vibe-planning comme un neuneu ! (devinez ce que j'ai fait ^^)

```bash
npx task-o-matic@latest tasks split  --all --stream \
  --reasoning 4096 \
  --ai "xiaomi/mimo-v2-flash:free;reasoning=4096"\
  --ai "nvidia/nemotron-3-nano-30b-a3b:free;reasoning=4096" \
  --ai "z-ai/glm-4.7;reasoning=4096" \
  --combine-ai "anthropic/claude-4.5-sonnet;reasoning=4096"
```

Je pense pas que Claude soit n√©cessaire l√†, mais j'avais mes $3 de cr√©dit openrouter √† br√ªler avant la fin du mois donc bon... on sort l'artillerie lourde !

Et bim, 30 centimes de plus ! mais maintenant... on est vraiment pr√™ts ! Presque...

#### AGENTS.md

Ou CLAUDE, ou GEMINI, d√©pend de votre outil de code assit√© pr√©f√©r√©, Pour moi, c'est opencode je vais lancer le prompt /init dans opencode !

```bash
opencode run "/init"
```

c'est pas vraiment obligatoire, mais c'est quand meme une bonne histoire d'eviter les plus grosse conneries et de recentrer quelques comportements pour certaines choses, du coup, on rajoute ca :

* Pas lancer le server de dev
* pas d√©marrer de processus en arri√®re-plan/de longue dur√©e
* pas utiliser `:any`, `as any` ou `any` comme type, je veux une base de code typesafe
* avant de dire "Jai finiiiii", `bun run check-types` et `bun run build` doivent tous les deux r√©ussir
* corriger les erreurs LSP quand elles arrivent
* utiliser les composants shadcn autant que possible
* rester DRY et pr√©f√©rer cr√©er des composants personnalis√©s et des utilitaires que r√©p√©ter du code
* utiliser les comp√©tences adapt√©es quand c'est possible (la comp√©tence frontend-design est obligatoire quand on travaille sur le frontend)
* que c'est un projet d'app statique qui sera h√©berg√© sur github pages
* utiliser Context7 MCP au cas o√π plus de documentation ou une documentation √† jour est n√©cessaire (car la connaissance est certainement en retard √† cause de la date limite de training)

```bash
opencode run "Add the following to the AGENTS.md, remove any conflicting instructions you created, these take precedence :
* Not to run the dev server
* not start background/long running processes
* not to use `:any`, `as any` or `any` as type, I want a typesafe codebase
* before claiming success, `bun run check-types` and `bun run build` must both succeed
* to fix LSP error when they happen
* use shadcn component has much as possible
* stay DRY and prefer creating custom components and utilities than repeat code
* use adapted skills when possible (frontend-design skill is mandatory when working on the frontend)
* that it is a static app project that will be hosted on github pages
* to use Context7 MCP in case more documentation or up to date documentation is needed (as the knowledge is certainly behind because of cutoff training date)" -c
```

J'ajouterai peut-√™tre plus pendant que l'agent travaille si je vois des erreurs r√©currentes et des gourdineries qui arrivent trop souvent (... si je regarde passer le train... c'est pas dit XD)

#### Et maintenant, on fait rouler Eric...

Bon les gars, cachez vos parents car on va laisser Eric y aller !

Parce que je suis El CheapoDev DelBrokeCasa, je vais utiliser mon GLM coding plan pour coder, comme √ßa, je vais pas eclater ma tirelire et c'est assez intelligent pour un truc comme √ßa je pense !

Si vous pr√©f√©rez claude code, utilisez l'option `--tool claude`. (ou codex/gemini/kilo) mais opencode est plus sympa car il stream le contenu (en partie en tout cas) donc vous voyez ce qui se passe (meuuuuh !) !

```bash
npx task-o-matic@latest tasks execute-loop --include-prd \
  --plan --plan-model zai-coding-plan/glm-4.7 \
  --review --review-model zai-coding-plan/glm-4.7 \
  --model zai-coding-plan/glm-4.7 \
  --validate "npm run check-types && npm run build"
```

Et maintenant, vous attendez, un caf√©, un gouter, un peu plus de caf√©, le d√Æner et pis le p'tit d√©j probablement, √ßa va prendre un moment ^^

## Vous en pensez quoi ? On va o√π apr√®s ?

Donc c'est la fin (de cet article) ! Vous en pensez quoi ? vous avez essay√© une technique similaire ? C'est quoi le plan apr√®s, parce que bon, vu comme c'est parti...

Si vous √™tes int√©ress√©s, le repo peut √™tre trouv√© sur [tiny-till github](https://github.com/DimitriGilbert/tiny-till)

et pour [task-o-matic](https://task-o-matic.dev) vous trouverez [le repo ici](https://github.com/DimitriGilbert/task-o-matic)
11:T11ee,
Hello everyone, I‚Äôd like to introduce LiteChat, a project I created.

[Repository](https://github.com/DimitriGilbert/LiteChat) and download link [here](https://github.com/DimitriGilbert/LiteChat/releases) (available in French, English, Spanish, Italian, and German).

LiteChat is an AI chat application I built to interact with both local and remote LLMs‚Äîall within your browser. It‚Äôs designed with a "local-first" philosophy, requiring only an HTTP server to run, with everything else handled in your browser! No tracking, no accounts‚Äîyou bring your own API keys. Data is saved in an IndexedDB database, and you can synchronize your conversations using Git.

Yes, entirely in the browser! üòõ To make this work, I also implemented a virtual file system (again, all in the browser). You can clone repos and include files from the VFS in your conversations! But manually selecting files was tedious, so I integrated tools for managing the VFS and Git.

With the core architecture in place, I added support for HTTP MCP servers. However, standard stdio servers were still missing, so I also built a bridge (rewritten by AI from `mcp-proxy`) to make them work (you can deploy it anywhere, but it‚Äôs not secure!).

Sure, AI is fun, but I was getting tired of text-only interactions. So, I added support for Mermaid diagrams and HTML forms (now you don‚Äôt even need to think about how to phrase your requests!). Sure, Mermaid diagrams aren‚Äôt the prettiest, but since I added a workflow module with visualizations based on [reactflow.dev](https://reactflow.dev), I also included a way for LLMs to generate them! And since plain text isn‚Äôt very engaging, there‚Äôs also a "Beat" block that uses [strudel.cc](http://strudel.cc/) to add auditory feedback.

Testing was getting repetitive, so I created a prompt library with templates‚Äînow you just fill in a form! (Okay, maybe I also needed it for workflows‚Ä¶)

What‚Äôs an Agent, you ask? It‚Äôs a system prompt, tools, and task-specific prompts! So, you also get a library for those.

Prompts and agents can integrate into workflows (that‚Äôs what they‚Äôre designed for!), but you also have transformation steps, user code execution, and custom prompts to facilitate transitions.

As you might have guessed, if I have a way to execute code in workflows, why not run AI-generated code? Yes, you can! In Python or JavaScript. And if you‚Äôre feeling adventurous, you can run JavaScript in "unsafe" mode (`eval` and all‚Äîthat‚Äôs code for "yolo" üòÜ). This can produce cool stuff, like [this one-shot Three.js scroll shooter](https://dimitrigilbert.github.io/racebench/scroller/index.html). You can export it in one click (the template is a bit ugly, but I‚Äôll improve it!).

To avoid losing context, all these UI blocks can be "activated" (or rather, suggested) using rules. Of course, you can add your own rules! There‚Äôs even a button to ask the AI to pick the best ones for your current prompt.

You also get the usual regenerate (with a different model if you like) and forking options. You can even edit responses manually to remove unnecessary parts. Code blocks are also editable with syntax highlighting for common languages (but no autocompletion or fancy features‚Äîlet‚Äôs not push grandma into the bushes!).

To top it all off, you can organize AI "races" with an unlimited number of participants (though that depends on your budget, haha). It‚Äôs great for benchmarking or seeing which model will replace us first. I even built a small tool that takes exported race conversations and turns them into a benchmark site (currently more focused on the JS execution block: [https://dimitrigilbert.github.io/racebench/scroller/index.html](https://dimitrigilbert.github.io/racebench/scroller/index.html) for the "game" mentioned earlier).

I‚Äôm sure I forgot a few things, but you‚Äôve got the gist! üòä

The hosted version is on GitHub Pages, with no tracking and no accounts! You bring your own API keys. You probably won‚Äôt be able to use the hosted version for your local LLM due to HTTPS/HTTP restrictions, but as I said, you can [download it](https://github.com/DimitriGilbert/LiteChat/releases) and host it with a simple HTTP server. There are also localized versions for French, Italian, German, and Spanish. A short (and incomplete) tutorial playlist if you‚Äôre feeling lost: [https://www.youtube.com/playlist?list=PL5Doe56gCsNRdNyfetOYPQw_JkPHO3XVh](https://www.youtube.com/playlist?list=PL5Doe56gCsNRdNyfetOYPQw_JkPHO3XVh)

I hope you enjoy it, and constructive feedback is greatly appreciated! üòä

---
12:T156f,
Salut a tous, pour mon premier post, je vous presente LiteChat dont je sui le cr√©ateur.

[Repo](https://github.com/DimitriGilbert/LiteChat) et ici [pour t√©l√©charger](https://github.com/DimitriGilbert/LiteChat/releases) (en francais, anglais, espagnol, italien et allemand).



C‚Äôest un chat IA que j‚Äôai cr√©√© pour pouvoir utiliser des LLM locaux et distant, le tout dans votre navigateur. Il est con√ßu pour √™tre ‚Äúlocal first‚Äù et n‚Äôa besoin que d‚Äôun serveur HTTP pour fonctionner, tout le reste c‚Äôest dans votre navigateur ! Pas de tracking, pas de compte, vous venez avec vos cl√© d‚ÄôAPI ! Les donn√©es sont sauvegard√©es dans une base de donn√©es IndexeDB et vous pouvez synchroniser vos conversations en utilisant git.

Oui, dans le navigateur :P Pour ce faire, j‚Äôai aussi d√ª impl√©menter un syst√®me de fichiers virtuel (oui, toujour dans le navigateur en utilisant ). Vous avez donc acc√®s aux deux, tant qu‚Äôa faire ! Vous pouvez cloner un repo et joindre des fichiers du VFS dans vos conversations !

Mais comme la s√©lection manuelle des fichiers, c‚Äô√©tait une corv√©e, j‚Äôai int√©gr√© des outils pour le VFS et git !

Ensuite, comme l‚Äôarchitecture de base √©tait l√†, et bien j‚Äôai ajout√© le support des serveurs MCP HTTP, mais il manquait toujours les serveurs stdio ‚Ä¶, donc j‚Äôai aussi ‚Äúfait‚Äù un bridge (r√©√©crit par l‚ÄôIA √† partir de mcp-proxy ) pour les utiliser (vous pouvez le d√©ployer o√π vous voulez mais ce n‚Äôest pas s√©curis√© !)

Apres, c‚Äôest bien mignon l‚ÄôIA, mais j‚Äôen avais un peu marre du texte seul, du coup, j‚Äôai ajout√© le support des diagrammes Mermaid et des formulaires HTML (Comme ca on a meme plus besoin de r√©fl√©chir a quoi lui dire a la machine !). Bon‚Ä¶ apr√®s‚Ä¶les diagrammes Mermaid c‚Äôest un peu moche, et comme j‚Äôai ajout√© un module de workflow avec des visualisations bas√©e sur [https://reactflow.dev/], j‚Äôai aussi ajout√© un moyen pour les LLM de vous en cr√©er ! Et puis comme le text ca fait pas beaucoup de bruit, il y a aussi in block ‚ÄúBeat‚Äù qui utilise (http://strudel.cc/) pour aussi en profiter de maniere auditive !

Et puis bon, en testant, toujours taper les m√™mes prompts avec juste quelques diff√©rences c‚Äô√©tait lourd aussi, j‚Äôai donc fait un module de biblioth√®que de prompts avec des mod√®les pour qu‚Äôy ai plus qu‚Äôa remplir un formulaire ;) (peut etre aussi que j‚Äôen avais besoin pour les workflows‚Ä¶)

‚ÄòPis, un Agents, c‚Äôest quoi ? hein ? bah un prompt syst√®me, des outils et des prompts sp√©cifiques pour les t√¢ches ! Donc √ßa aussi vous en avez une librairie !

Les prompts et les agents peuvent s‚Äôint√©grer dans les workflows (duh, ils √©taient faits pour √ßa !) mais vous avez aussi des √©tapes de ‚Äútransformation‚Äù/ex√©cution de code utilisateur/‚Äúprompt personnalis√©‚Äù pour faciliter le transit !

Comme vous l‚Äôavez peut-√™tre devin√©, si j‚Äôai une forme d‚Äôex√©cution de code pour les workflows, Est ce que je pourrais t‚Äôy donc pas faire tourner le code g√©n√©r√© par l‚ÄôIA, hein ? Eeeeh bha SI ! En Python ou JavaScript Et m√™me que si vous √™te un dinguo, vous pouvez faire tourner le js en mode ‚Äúunsafe‚Äù (eval et yolo XD) du coup ca peut produire des trucs (comme ce one shot threejs scroll shooter ) que vous pouvez exporter en 1 clic (le template est moche mais je vais y travailler !)

Afin de n‚Äôpas completement obliterer le context, tous ces jolis blocs d‚ÄôUI peuvent √™tre ‚Äúactiv√©s‚Äù (plut√¥t sugg√©r√©s ^^) en utilisant des r√®gles. Evidement, vous pouvez ajouter vos r√®gles a vous ! Et meme que vous avez un bouton pour demander a l‚ÄôIA de les choisir pour vous pour votre prompt actuel !

Bien s√ªr, vous avez les habituels regen (avec un mod√®le diff√©rent si le coeur vous en dis) et forking, mais vous pouvez aussi modifier une r√©ponse a la main pour d√©gager l‚Äôinutile. D‚Äôailleur, Les blocs de code sont aussi modifiable avec la coloration syntaxique pour les langages les plus courants, (mais pas d‚Äôauto-compl√©tion ou autre truc de bogoss, on va pas poussez m√©m√© dans les orties !).

Pour couronner le tout, vous pouvez organiser des courses d‚ÄôIA avec un nombre illimit√© de participants (ca dependra de la profondeur de votre portefeuille :P). C‚Äôest cool pour faire des benchmarks ou quand on veut voir laquelle des machines prendra notre place en premier‚Ä¶ J‚Äôai m√™me fait un petit outil qui prend une conversation de course export√©e et qui cr√©e un mini site de benchmark (plus cibl√© sur le bloc d‚Äôex√©cution JS pour l‚Äôinstant https://dimitrigilbert.github.io/racebench/scroller/index.html pour le ‚Äújeu‚Äù d‚Äôavant)

J‚Äôoublie certainement quelques bricoles, mais vous avez compris l‚Äôessentiel ^^

La version h√©berg√©e est sur les pages GitHub et il n‚Äôy a pas de tracking, pas de compte ! Vous apportez vos propres cl√©s API ! Vous ne pourrez probablement pas utiliser la version h√©berg√©e pour votre LLM local √† cause des restrictions https/http, mais comme je l‚Äôai dit, vous pouvez t√©l√©charger https://github.com/DimitriGilbert/LiteChat/releases et h√©berger avec un simple serveur HTTP. Vous avez m√™me des versions localis√©es pour le fran√ßais, l‚Äôitalien, l‚Äôallemand et l‚Äôespagnol. Une petite playlist (tr√®s incompl√®te) de tutoriels si vous vous sentez un peu perdu https://www.youtube.com/playlist?list=PL5Doe56gCsNRdNyfetOYPQw_JkPHO3XVh

J‚Äôesp√®re que vous appr√©cierez et les commentaires constructifs sont grandement appr√©ci√©s :D13:T1e1b,
_This content has been generated by GLM 4.7 AI model_

So, you know how everyone's benchmarking LLMs these days? Like, "oh look, this model scored 87.3% on this synthetic test suite" or whatever. Boring, right? I mean, who cares if a model can reverse a string perfectly when you'll never ask it to reverse a string in real life? üòõ

That's why I built **RaceBench** ‚Äî a benchmarking tool that actually tests something useful. How well different LLMs can write real, working JavaScript code. Not some "hello world" nonsense, but actual creative coding tasks ‚Äî games, visualisations, interactive stuff you might actually want to build.

## The idea (because every project needs an origin story)

One day I was curious: if I ask different AI models to write the same game, which one would actually produce playable code? Not "technically correct but crashes on the third frame" code, but stuff you could run and enjoy.

So I created a benchmark where I ask models to build a 2D scroll shooter game using Three.js. Not exactly "easy mode" for an AI ‚Äî it requires understanding 3D graphics, game loops, user input, and making everything work together without blowing up the browser. Spoiler: some models handle it beautifully, others... well, let's say it's educational to watch them try üòä

## What it actually does

RaceBench is a static HTML dashboard that displays results from testing various LLMs on JavaScript code generation. Here's the cool part:

- **Real-world benchmarking** ‚Äî we test creative coding tasks, not synthetic "reverse this string" nonsense
- **Executable results** ‚Äî every model's output can be run directly. See the code, click preview, judge for yourself
- **Multi-dimensional analysis** ‚Äî not just "did it work?" We track prompt tokens, completion tokens, TTFT (time to first token), generation time, and success rate
- **Educational value** ‚Äî compare how different models structure their code, their approaches to the same problem, and learn from the variety (even the bad ones!)

## How it works (the technical bits, minus the boring parts)

### The setup

Everything's static ‚Äî pure HTML/CSS/JS with no backend. Deploy it anywhere. GitHub Pages? Sure. Netlify? Why not. Your Raspberry Pi at home? Absolutely. The whole thing weighs about as much as a small hamster üêπ

```bash
git clone https://github.com/DimitriGilbert/racebench.git

# Deploy to GitHub Pages (or your jungle site of choice)
# I mean, whatever floats your boat
```

### The test case

The benchmark uses a 2D scroll shooter game built with Three.js as the test scenario. Each model gets the same prompt and has to produce working code. We extract the last ````runjs` code block from each response (because models love to chat before giving you the goods, you know?).

### What we track

For each model, RaceBench captures:

- **Prompt tokens** ‚Äî how much it needed to understand what we wanted
- **Completion tokens** ‚Äî how verbose it got with the solution
- **TTFT** (Time To First Token) ‚Äî responsiveness metrics
- **Generation time** ‚Äî total time to produce the code
- **Success rate** ‚Äî did it actually run? Or did it explode?

### The UI (because I care about aesthetics, surprisingly)

Nice dark mode UI that doesn't look like it was designed by a sysadmin in 2003. Two view modes:

- **Card grid** ‚Äî pretty cards for each model with key metrics at a glance
- **Sortable table** ‚Äî for when you want to data-nerd out and sort by TTFT or token count

Click any model and you get:
- Live preview (opens in a new tab, runs the actual code)
- Raw response view (see exactly what the model said)
- All the metrics broken down

## Versions (because software never ends)

### v1: The brave beginning

Tested 18 models. Learned a lot. Realised some models are impressively creative while others are... trying their best. Let's put it that way. üòõ

### v2: The refinement

Narrowed it down to 10 models. Better metrics extraction. Cleaner UI. Still watching models struggle with Three.js in entertaining ways.

## Tech stack (kept it simple, you're welcome)

- **Node.js** ‚Äî because JavaScript is life
- **Tailwind CSS (via CDN)** ‚Äî styling without the build step headaches
- **Simple-Datatables** ‚Äî for when you want sortable tables but don't want to write them yourself
- **Three.js (dynamic)** ‚Äî loaded on demand for the live previews

Minimal dependencies. No complex build pipelines. Just works. I like that philosophy.

## The models (participants in the chaos)

We've tested a bunch of the big names:

- Claude Sonnet 4
- DeepSeek R1
- GPT-4.1
- Gemini 2.5
- Grok 4
- Kimi K2
- Qwen3
- ...and more

Each one brings its own personality to the code. Some are terse and efficient. Others are verbose and over-engineered. A few produce code that makes you question everything you know about software development. All educational in their own way üòä

## See it in action

Don't just take my word for it ‚Äî go play with the actual dashboard:

**[Live Demo](https://dimitrigilbert.github.io/racebench/scroller/index.html)**

Click around. Run some code. See which model you'd want on your team. Spoiler: the answer might surprise you (or it might be exactly what you expect. LLMs are weird like that).

## What makes this different from other benchmarks?

I'm glad you asked (I know you didn't, but pretend you did) üòõ

1. **Real tasks, not toy problems** ‚Äî we test actual creative coding, not "sort this array" nonsense
2. **You can run the results** ‚Äî not just a score, actual working code you can inspect and execute
3. **No server needed** ‚Äî static HTML means you can host it anywhere or even run it locally
4. **Transparent metrics** ‚Äî see exactly what we measured and how
5. **Educational** ‚Äî compare approaches, learn patterns, see what works (and what definitely doesn't)

## What I learned (because building stuff teaches you things)

- Some models are surprisingly good at 3D graphics code
- Others clearly haven't seen many Three.js examples in their training
- TTFT doesn't always correlate with code quality
- Verbose models aren't necessarily better ‚Äî sometimes they're just... chatty
- The best model depends on what you value: speed, efficiency, code clarity, or just getting something that runs

## Future plans (the "if I get around to it" section)

I'd like to:

- Add more test cases (different types of creative coding challenges)
- Test more models (new ones keep popping up like mushrooms)
- Improve the metrics (maybe add code quality scoring?)
- Add a "leaderboard" view for those who love rankings
- Maybe, possibly, contribute the data to some broader benchmark effort (if such a thing exists and wants this kind of real-world data)

But let's be honest ‚Äî I'll probably add a new test case before I remember to do any of that. Priorities, right? üòä

## Wrapping up

RaceBench isn't trying to be the definitive AI benchmark. It's just me trying to answer a simple question: "Which AI writes the best JavaScript code for real projects?" And having fun seeing them all try.

If you find it useful, great. If you learn something from the different code styles, even better. If you just enjoy clicking through live previews of AI-generated games and thinking "wow, that actually works," then I've done my job.

Feel free to fork it, add your own models, test different prompts. The more data, the better. Just don't ask me which model is "best" ‚Äî that depends on what you're building, your timeline, your budget, and probably the phase of the moon. LLMs are like that üòõ

**GitHub:** [DimitriGilbert/racebench](https://github.com/DimitriGilbert/racebench)

Now go race some models. May the best code win.

{{% goodbye %}}
14:T23a9,
_Ce contenu a √©t√© g√©n√©r√© par le mod√®le AI GLM 4.7 et not-ai-writer_

Alors, vous voyez comme tout le monde benchmarke des LLMs en ce moment ? Genre, "oh regarde, ce mod√®le a score 87,3% sur cette suite de tests synth√©tique" ou quoi que ce soit dans le genre. Ennuyeux, non ? Je veux dire, on s'en fiche de savoir si un mod√®le peut inverser une cha√Æne parfaitement quand on ne lui demandera jamais d'inverser une cha√Æne dans la vraie vie ? üòõ

C'est pour √ßa que j'ai cr√©√© **RaceBench** ‚Äî un outil de benchmark qui teste vraiment quelque chose d'utile. √Ä quel point diff√©rents LLMs peuvent √©crire du code JavaScript r√©el qui fonctionne. Pas des trucs de "hello world" nuls, mais de vraies t√¢ches de code cr√©atif ‚Äî des jeux, des visualisations, des trucs interactifs que vous pourriez vraiment vouloir construire.

## L'id√©e (parce que tout projet a besoin d'une histoire d'origine)

Un jour j'√©tais curieux: si je demande √† diff√©rents mod√®les d'IA d'√©crire le m√™me jeu, lequel produirait vraiment du code jouable ? Pas du code "techniquement correct mais qui plante √† la troisi√®me frame", mais des trucs qu'on pourrait lancer et appr√©cier.

Donc j'ai cr√©√© un benchmark o√π je demande aux mod√®les de construire un jeu de tir scroll 2D en utilisant Three.js. Pas vraiment "mode facile" pour une IA ‚Äî √ßa demande de comprendre la 3D, les boucles de jeu, les entr√©es utilisateur, et de faire tout fonctionner ensemble sans faire exploser le navigateur. Spoiler: certains mod√®les g√®rent √ßa magnifiquement, d'autres... enfin, disons que c'est √©ducatif de les essayer üòä

## Ce que √ßa fait vraiment

RaceBench est un dashboard HTML statique qui affiche les r√©sultats de tests de divers LLMs sur la g√©n√©ration de code JavaScript. Voici la partie cool:

- **Benchmarking r√©el** ‚Äî on teste des t√¢ches de code cr√©atif, pas des trucs synth√©tiques du genre "inverse cette cha√Æne"
- **R√©sultats ex√©cutables** ‚Äî la sortie de chaque mod√®le peut √™tre lanc√©e directement. Voyez le code, cliquez sur preview, jugez par vous-m√™me
- **Analyse multi-dimensionnelle** ‚Äî pas juste "√ßa a march√© ?" On tracke les tokens de prompt, les tokens de compl√©tion, le TTFT (temps jusqu'au premier token), le temps de g√©n√©ration, et le taux de succ√®s
- **Valeur √©ducative** ‚Äî comparez comment diff√©rents mod√®les structurent leur code, leurs approches au m√™me probl√®me, et apprenez de la vari√©t√© (m√™me des mauvais !)

## Comment √ßa marche (les bits techniques, moins les parties ennuyeuses)

### Le setup

Tout est statique ‚Äî du HTML/CSS/JS pur sans backend. D√©ployez-le n'importe o√π. GitHub Pages ? S√ªr. Netlify ? Pourquoi pas. Votre Raspberry Pi √† la maison ? Absolument. Le tout p√®se √† peu pr√®s le poids d'un petit hamster üêπ

```bash
git clone https://github.com/DimitriGilbert/racebench.git

# Deploy to GitHub Pages (or your jungle site of choice)
# I mean, whatever floats your boat
```

### Le cas de test

Le benchmark utilise un jeu de tir scroll 2D construit avec Three.js comme sc√©nario de test. Chaque mod√®le re√ßoit le m√™me prompt et doit produire du code qui fonctionne. On extrait le dernier bloc de code ```runjs de chaque r√©ponse (parce que les mod√®les adorent discuter avant de vous donner la marchandise, vous voyez ?).

### Ce qu'on tracke

Pour chaque mod√®le, RaceBench capture:

- **Prompt tokens** ‚Äî combien il en a eu besoin pour comprendre ce qu'on voulait
- **Completion tokens** ‚Äî √† quel point il a √©t√© verbeux avec la solution
- **TTFT** (Time To First Token) ‚Äî m√©triques de r√©activit√©
- **Generation time** ‚Äî temps total pour produire le code
- **Success rate** ‚Äî √ßa a vraiment tourn√© ? Ou √ßa a explos√© ?

### L'UI (parce que je me soucie de l'esth√©tique, √©tonnamment)

Une superbe interface dark mode qui n'a pas l'air d'avoir √©t√© con√ßue par un admin sys en 2003. Deux modes de vue:

- **Grille de cartes** ‚Äî de belles cartes pour chaque mod√®le avec les m√©triques cl√©s en un coup d'≈ìil
- **Tableau triable** ‚Äî pour quand vous voulez faire le data-nerd et trier par TTFT ou nombre de tokens

Cliquez sur n'importe quel mod√®le et vous obtenez:
- Live preview (s'ouvre dans un nouvel onglet, lance le vrai code)
- Vue de la r√©ponse brute (voyez exactement ce que le mod√®le a dit)
- Toutes les m√©triques d√©taill√©es

## Versions (parce que le logiciel ne se termine jamais)

### v1: Le d√©but courageux

Test√© 18 mod√®les. Appris beaucoup. R√©alis√© que certains mod√®les sont impressionnamment cr√©atifs tandis que d'autres sont... essaient leur meilleur. Mettons les choses comme √ßa. üòõ

### v2: Le raffinement

R√©duit √† 10 mod√®les. Meilleure extraction des m√©triques. UI plus propre. Toujours √† regarder les mod√®les gal√©rer avec Three.js de mani√®re amusante.

## Stack technique (gard√© simple, de rien)

- **Node.js** ‚Äî parce que JavaScript est la vie
- **Tailwind CSS (via CDN)** ‚Äî du style sans les maux de t√™te de build
- **Simple-Datatables** ‚Äî pour quand vous voulez des tableaux triables mais ne voulez pas les √©crire vous-m√™me
- **Three.js (dynamique)** ‚Äî charg√© √† la demande pour les previews en direct

D√©pendances minimales. Pas de pipelines de build complexes. Juste √ßa marche. J'aime cette philosophie.

## Les mod√®les (participants dans le chaos)

On a test√© plein de grands noms:

- Claude Sonnet 4
- DeepSeek R1
- GPT-4.1
- Gemini 2.5
- Grok 4
- Kimi K2
- Qwen3
- ...et plus

Chacun apporte sa propre personnalit√© au code. Certains sont laconiques et efficaces. D'autres sont verbeux et sur-ing√©nier√©s. Quelques-uns produisent du code qui vous fait remettre en question tout ce que vous savez sur le d√©veloppement logiciel. Tous √©ducatifs √† leur mani√®re üòä

## Voyez-le en action

Ne me croyez pas sur parole ‚Äî allez jouer avec le vrai dashboard:

**[D√©mo Live](https://dimitrigilbert.github.io/racebench/scroller/index.html)**

Cliquez autour. Lancez du code. Voyez quel mod√®le vous voudriez dans votre √©quipe. Spoiler: la r√©ponse pourrait vous surprendre (ou elle pourrait √™tre exactement ce que vous attendez. Les LLMs sont bizarres comme √ßa).

## Ce qui rend √ßa diff√©rent des autres benchmarks ?

Je suis ravi que vous demandiez (je sais que vous n'avez pas demand√©, mais faites semblant) üòõ

1. **Vraies t√¢ches, pas probl√®mes jouets** ‚Äî on teste du code cr√©atif r√©el, pas des trucs du genre "trie ce tableau"
2. **Vous pouvez lancer les r√©sultats** ‚Äî pas juste un score, du vrai code qui fonctionne que vous pouvez inspecter et ex√©cuter
3. **Pas de serveur n√©cessaire** ‚Äî HTML statique signifie que vous pouvez l'h√©berger n'importe o√π ou m√™me le lancer localement
4. **M√©triques transparentes** ‚Äî voyez exactement ce qu'on a mesur√© et comment
5. **√âducatif** ‚Äî comparez les approches, apprenez des patterns, voyez ce qui marche (et ce qui ne marche DEFINITIVEMENT pas)

## Ce que j'ai appris (parce que construire des trucs vous apprend des choses)

- Certains mod√®les sont √©tonnamment bons en code graphique 3D
- D'autres n'ont clairement pas vu beaucoup d'exemples Three.js dans leur entra√Ænement
- Le TTFT ne correlate pas toujours avec la qualit√© du code
- Les mod√®les verbeux ne sont pas n√©cessairement meilleurs ‚Äî parfois ils sont juste... bavards
- Le meilleur mod√®le d√©pend de ce que vous valorisez: vitesse, efficacit√©, clart√© du code, ou juste avoir quelque chose qui tourne

## Plans futurs (la section "si je m'y retrouve")

J'aimerais:

- Ajouter plus de cas de test (diff√©rents types de d√©fis de code cr√©atif)
- Tester plus de mod√®les (de nouveaux surgissent comme des champignons)
- Am√©liorer les m√©triques (peut-√™tre ajouter un scoring de qualit√© du code ?)
- Ajouter une vue "leaderboard" pour ceux qui adorent les classements
- Peut-√™tre, possiblement, contribuer les donn√©es √† un effort de benchmark plus large (si un tel truc existe et veut ce genre de donn√©es du monde r√©el)

Mais soyons honn√™tes ‚Äî j'ajouterai probablement un nouveau cas de test avant de me souvenir de faire quoi que ce soit de tout √ßa. Les priorit√©s, quoi ? üòä

## Conclusion

RaceBench n'essaie pas d'√™tre le benchmark AI d√©finitif. C'est juste moi qui essaie de r√©pondre √† une question simple: "Quelle IA √©crit le meilleur code JavaScript pour des projets r√©els ?" Et m'amuse √† les voir tous essayer.

Si vous le trouvez utile, super. Si vous apprenez quelque chose des diff√©rents styles de code, encore mieux. Si vous aimez juste cliquer √† travers des previews en direct de jeux g√©n√©r√©s par IA et penser "wow, √ßa marche vraiment," alors j'ai fait mon boulot.

N'h√©sitez pas √† le forker, ajouter vos propres mod√®les, tester diff√©rents prompts. Plus il y a de donn√©es, mieux c'est. Ne me demandez pas quel mod√®le est "le meilleur" ‚Äî √ßa d√©pend de ce que vous construisez, votre timeline, votre budget, et probablement la phase de la lune. Les LLMs sont comme √ßa üòõ

**GitHub:** [DimitriGilbert/racebench](https://github.com/DimitriGilbert/racebench)

Maintenant allez faire la course √† quelques mod√®les. Que le meilleur code gagne.

{{% goodbye %}}
15:T1d38,
_This content has been generated by GLM 4.7 AI model_

So. Who here has **project ideas**? And among these ideas, some turn into... uh, actual tasks, right?

Ok ok, so, get me a structured plan, broken down tasks, and anything resembling order. Like, right NOW!

Yeah...it's ok. We've all been there üòõ.

The wasteland of unorganized thoughts is a dangerous place. But worry not ‚Äî I built us a bunker.

## Welcome to the Bunker üèöÔ∏è

[Task-O-Matic](https://github.com/DimitriGilbert/task-o-matic) is an AI-powered task management system with a post-apocalyptic survival bunker theme (because why not?). It takes your chaotic project ideas and turns them into structured, executable task lists through AI assistance.

Think of it as your personal task commander in the aftermath of project anarchy.

## What it actually does üìã

Task-O-Matic isn't state of the art ‚Äî I'm not a bloody AI researcher ‚Äî but it *does* work. And it's rather useful for us bunker dwellers:

- **Idea to PRD** : Take your half-baked idea and let AI turn it into a proper Product Requirements Document
- **PRD to tasks** : Break that PRD into actual, doable tasks
- **AI-powered enhancement** : Let the AI improve, split, and plan your tasks
- **Multi-AI generation** : Run multiple AI models in parallel and compare results (battle royale style)
- **External tool integration** : Connect to Opencode, Claude Code, Gemini CLI, and more
- **Benchmarking** : Test different AI models with git worktree isolation
- **Existing projects** : Attach Task-O-Matic to projects you already started (`init attach`)
- **Local-first** : Everything lives in `.task-o-matic` (the bunker's archives)
- **Hierarchical breakdown** : Subtasks, sub-subtasks, sub-sub-sub... you get the idea
- **Streaming responses** : Watch the AI think with reasoning tokens

## The tech stack üîß

Built with tools that survived the apocalypse:

- **TypeScript** ‚Äî type safety is survival safety
- **Bun** ‚Äî fast (needed when running from generators)
- **Vercel AI SDK v6** ‚Äî the magic behind the AI curtain
- **Commander.js** ‚Äî CLI parsing (I know a thing or two about CLI tools...)
- **Inquirer.js** ‚Äî interactive prompts (for bunker decisions)
- **Zod** ‚Äî schema validation (don't want corrupted data in the archives)

## Quick start ‚ö°

Oh, I know you're impatient. (Me too...)

```bash
git clone https://github.com/DimitriGilbert/task-o-matic.git
cd task-o-matic
bun install
bun link
task-o-matic init attach
task-o-matic prd generate "Build a survival bunker dashboard"
task-o-matic prd parse
task-o-matic task enhance --all
task-o-matic task execute --ai claude
```

Done.

## Installation üî®

```bash
git clone https://github.com/DimitriGilbert/task-o-matic.git
cd task-o-matic
bun install
bun link
```

Or via npm if you prefer the old ways: `npm install -g @dimitrigilbert/task-o-matic`

## Configuration üîë

Task-O-Matic needs API keys. Set them as environment variables:

```bash
export OPENAI_API_KEY="your-key"
export ANTHROPIC_API_KEY="your-key"
export GOOGLE_API_KEY="your-key"
export OPENROUTER_API_KEY="your-key"
```

Put these in `.env` or your shell profile. Don't commit them to git, obviously.

## How it works üöÄ

### Initialize

```bash
task-o-matic init attach    # attach to existing project
task-o-matic init new my-project   # or create new
```

Creates a `.task-o-matic` directory. Like your bunker's storage room.

### Generate a PRD

```bash
task-o-matic prd generate "I want to build a personal finance tracker that works offline, syncs when online, and has dark mode because bunker lights are harsh"
```

The AI will interview you (via inquirer prompts) to clarify requirements and generate a proper PRD.

### Parse into tasks

```bash
task-o-matic prd parse
```

Breaks down your PRD into hierarchical tasks, subtasks, sub-subtasks. Organized by priority and dependencies.

### Enhance and plan

```bash
task-o-matic task enhance --all
task-o-matic task plan
task-o-matic task split --id task-123
```

### Execute

```bash
task-o-matic task execute --id task-123 --ai claude
task-o-matic task execute --all --ai claude,gemini,gpt4
task-o-matic task execute --id task-123 --tool opencode
```

The AI reads the task description and context, generates code or documentation. You review before committing.

## Multi-AI ü§ñ

Run multiple AI models in parallel:

```bash
task-o-matic task generate-multi \
  --id task-123 \
  --providers claude,gpt4,gemini \
  --compare
```

See different approaches. Verify the best solution. Like having a council of AI advisors in your bunker üòä

## Benchmarking üìä

```bash
task-o-matic benchmark run \
  --models claude,gpt4,gemini \
  --task-set my-tasks
task-o-matic benchmark results
task-o-matic benchmark compare --run-id run1,run2
```

Uses git worktrees. Clean environments. No contamination!

## Local-first storage üíæ

Everything lives in `.task-o-matic/`:

```
.prds/           # Product Requirements Documents
tasks/          # Task definitions and status
history/        # Execution history
benchmarks/     # Benchmark results
config.json     # Your configuration
```

No cloud. No data leaving your bunker (unless you choose to). Your data, your rules.

## Streaming with reasoning tokens üß†

```bash
task-o-matic task execute --id task-123 --stream
```

Shows reasoning tokens as they're generated. Fascinating to see how the AI approaches problems (sometimes in ways I wouldn't expect, which is good).

## Why the bunker theme? ü§î

I know, I know. It's weird. But here's the thing:

1. It's fun üòõ
2. Projects feel like chaotic wastelands
3. A structured system feels like building a safe shelter
4. "Bunker" is a great metaphor for local-first, isolated development
5. Why not?

Also, calling users "citizens" and projects "missions" makes me chuckle. If it makes you smile too, then it's worth it.

## Current status üöß

Task-O-Matic is at **v0.1.5-beta.1**. Things might break. Features might change. But the core functionality works.

If you find bugs (and I'm sure there are nests of them), open an issue on [GitHub](https://github.com/DimitriGilbert/task-o-matic/issues). I'm the only user right now, so finding bugs is becoming... challenging üòä

## What's next? üîÆ

- More AI provider integrations
- Web UI (because some people prefer graphical interfaces)
- Team collaboration (multi-bunker coordination)
- Plugin system for custom tools
- Better dependency visualization
- Task templates for common project types
- CI/CD integration

Subject to change based on what the wasteland throws at us.

## Get involved! ü§ù

This is a side project I built because I needed it. If you find it useful, that's awesome. If you want to contribute, even better!

Star the repo. Open issues. Submit PRs. Share it with other bunker dwellers.

## Final thoughts üí≠

Task-O-Matic isn't going to revolutionize project management or anything grand. It's just a tool I built to help me (and hopefully you) turn chaotic ideas into organized, executable tasks.

It's not perfect. It's not the most elegant code I've ever written. But it works. It's useful. And it's mine.

If you're drowning in project chaos and need a structured approach to breaking things down, give it a shot. Worst case, you'll have a cool CLI tool with a bunker theme.

Best case, you'll actually ship that project you've been thinking about for months.

Stay safe out there in the wasteland, citizen. And happy coding!

---

{{% goodbye %}}
16:T2121,
_Ce contenu a √©t√© g√©n√©r√© par le mod√®le AI GLM 4.7 et not-ai-writer_

Alors. Qui ici a des **id√©es de projets** ? Et parmi ces id√©es, certaines se transforment en... euh, vraies t√¢ches, non ?

Ok ok, alors, donnez-moi un plan structur√©, des t√¢ches d√©taill√©es, et tout ce qui ressemble √† de l'ordre. Genre, MAINTENANT !

Ouais...c'est bon. On est tous pass√©s par l√† üòõ.

Le wasteland des pens√©es d√©sorganis√©es est un endroit dangereux. Mais pas de souci ‚Äî je nous ai construit un bunker.

## Bienvenue dans le Bunker üèöÔ∏è

[Task-O-Matic](https://github.com/DimitriGilbert/task-o-matic) est un syst√®me de gestion de t√¢ches propuls√© par l'IA avec un th√®me de bunker de survie post-apocalyptique (parce que pourquoi pas ?). Il prend vos id√©es de projet chaotiques et les transforme en listes de t√¢ches structur√©es et ex√©cutables gr√¢ce √† l'IA.

Consid√©rez √ßa comme votre commandant de t√¢ches personnel dans les suites d'une anarchie de projet.

## Ce que √ßa fait vraiment üìã

Task-O-Matic n'est pas state of the art ‚Äî je suis pas un putain de chercheur en IA ‚Äî mais √ßa *marche*. Et c'est plut√¥t utile pour nous, habitants de bunker :

- **Id√©e vers PRD** : Prenez votre id√©e √† moiti√© cuite et laissez l'IA la transformer en un vrai Product Requirements Document
- **PRD vers t√¢ches** : D√©coupez ce PRD en vraies t√¢ches faisables
- **Am√©lioration propuls√©e par l'IA** : Laissez l'IA am√©liorer, diviser et planifier vos t√¢ches
- **G√©n√©ration multi-IA** : Lancez plusieurs mod√®les d'IA en parall√®le et comparez les r√©sultats (style battle royale)
- **Int√©gration d'outils externes** : Connectez-vous √† Opencode, Claude Code, Gemini CLI, et plus
- **Benchmarking** : Testez diff√©rents mod√®les d'IA avec l'isolation git worktree
- **Projets existants** : Attachez Task-O-Matic √† des projets que vous avez d√©j√† commenc√©s (`init attach`)
- **Local-first** : Tout vit dans `.task-o-matic` (les archives du bunker)
- **D√©coupage hi√©rarchique** : Sous-t√¢ches, sous-sous-t√¢ches, sous-sous-sous... vous voyez l'id√©e
- **R√©ponses en streaming** : Regardez l'IA r√©fl√©chir avec les tokens de raisonnement

## La stack technique üîß

Construit avec des outils qui ont surv√©cu √† l'apocalypse :

- **TypeScript** ‚Äî la s√ªret√© des types, c'est la s√ªret√© de survie
- **Bun** ‚Äî rapide (n√©cessaire quand on fui les g√©n√©rateurs)
- **Vercel AI SDK v6** ‚Äî la magie derri√®re le rideau IA
- **Commander.js** ‚Äî parsing CLI (je sais un truc ou deux sur les outils CLI...)
- **Inquirer.js** ‚Äî prompts interactifs (pour les d√©cisions du bunker)
- **Zod** ‚Äî validation de sch√©ma (pas de donn√©es corrompues dans les archives)

- D√©marrage rapide ‚ö°

Oh, je sais que vous √™tes impatient. (Moi aussi...)

```bash
git clone https://github.com/DimitriGilbert/task-o-matic.git
cd task-o-matic
bun install
bun link
task-o-matic init attach
task-o-matic prd generate "Build a survival bunker dashboard"
task-o-matic prd parse
task-o-matic task enhance --all
task-o-matic task execute --ai claude
```

Voil√†.

## Installation üî®

```bash
git clone https://github.com/DimitriGilbert/task-o-matic.git
cd task-o-matic
bun install
bun link
```

Ou via npm si vous pr√©f√©rez les vieilles m√©thodes : `npm install -g @dimitrigilbert/task-o-matic`

## Configuration üîë

Task-O-Matic a besoin de cl√©s API. Configurez-les comme variables d'environnement :

```bash
export OPENAI_API_KEY="your-key"
export ANTHROPIC_API_KEY="your-key"
export GOOGLE_API_KEY="your-key"
export OPENROUTER_API_KEY="your-key"
```

Mettez √ßa dans `.env` ou votre profil de shell. √âvidemment, les commit pas dans git.

## Comment √ßa marche üöÄ

### Initialiser

```bash
task-o-matic init attach    # attacher √† un projet existant
task-o-matic init new my-project   # ou en cr√©er un nouveau
```

Cr√©e un r√©pertoire `.task-o-matic`. Comme la salle de stockage de votre bunker.

### G√©n√©rer un PRD

```bash
task-o-matic prd generate "I want to build a personal finance tracker that works offline, syncs when online, and has dark mode because bunker lights are harsh"
```

L'IA va vous interviewer (via les prompts inquirer) pour clarifier les requis et g√©n√©rer un vrai PRD.

### Parser en t√¢ches

```bash
task-o-matic prd parse
```

D√©coupe votre PRD en t√¢ches hi√©rarchiques, sous-t√¢ches, sous-sous-t√¢ches. Organis√©es par priorit√© et d√©pendances.

### Am√©liorer et planifier

```bash
task-o-matic task enhance --all
task-o-matic task plan
task-o-matic task split --id task-123
```

### Ex√©cuter

```bash
task-o-matic task execute --id task-123 --ai claude
task-o-matic task execute --all --ai claude,gemini,gpt4
task-o-matic task execute --id task-123 --tool opencode
```

L'IA lit la description de la t√¢che et le contexte, g√©n√®re du code ou de la documentation. Vous reviewez avant de committer.

## Multi-IA ü§ñ

Lancez plusieurs mod√®les d'IA en parall√®le :

```bash
task-o-matic task generate-multi \
  --id task-123 \
  --providers claude,gpt4,gemini \
  --compare
```

Voyez diff√©rentes approches. V√©rifiez la meilleure solution. Comme avoir un conseil d'experts IA dans votre bunker üòä

## Benchmarking üìä

```bash
task-o-matic benchmark run \
  --models claude,gpt4,gemini \
  --task-set my-tasks
task-o-matic benchmark results
task-o-matic benchmark compare --run-id run1,run2
```

Utilise les git worktrees. Environnements propres. Pas de contamination !

## Stockage local-first üíæ

Tout vit dans `.task-o-matic/` :

```
.prds/           # Product Requirements Documents
tasks/          # D√©finitions de t√¢ches et statuts
history/        # Historique d'ex√©cution
benchmarks/     # R√©sultats de benchmark
config.json     # Votre configuration
```

Pas de cloud. Pas de donn√©es qui quittent votre bunker (sauf si vous le choisissez). Vos donn√©es, vos r√®gles.

## Streaming avec tokens de raisonnement üß†

```bash
task-o-matic task execute --id task-123 --stream
```

Affiche les tokens de raisonnement au fur et √† mesure qu'ils sont g√©n√©r√©s. Fascinant de voir comment l'IA aborde les probl√®mes (parfois de fa√ßons que je n'aurais pas attendues, ce qui est bon).

## Pourquoi le th√®me bunker ? ü§î

Je sais, je sais. C'est bizarre. mais voil√† le truc :

1. C'est fun üòõ
2. Les projets ressemblent √† des wastelands chaotiques
3. Un syst√®me structur√© ressemble √† la construction d'un abri s√ªr
4. "Bunker" est une super m√©taphore pour le dev local-first et isol√©
5. Pourquoi pas ?

Et puis, appeler les utilisateurs "citoyens" et les projets "missions" me fait pouffer. Si √ßa vous fait sourire aussi, alors √ßa en vaut la peine.

## Statut actuel üöß

Task-O-Matic est en **v0.1.5-beta.1**. Des trucs peuvent casser. Des features peuvent changer. Mais la fonctionnalit√© core marche.

Si vous trouvez des bugs (et je suis s√ªr qu'il y a des nids), ouvrez une issue sur [GitHub](https://github.com/DimitriGilbert/task-o-matic/issues). Je suis le seul utilisateur pour l'instant, donc trouver des bugs devient... challengeant üòä

## Et apr√®s ? üîÆ

- Plus d'int√©grations de providers IA
- Web UI (parce que certains pr√©f√®rent les interfaces graphiques)
- Collaboration d'√©quipe (coordination multi-bunker)
- Syst√®me de plugins pour outils custom
- Meilleure visualisation des d√©pendances
- Templates de t√¢ches pour types de projets communs
- Int√©gration CI/CD

Sujet √† changement selon ce que le wasteland nous balance.

## Impliquez-vous ! ü§ù

C'est un side project que j'ai construit dont j'avais besoin. Si vous le trouvez utile, c'est awesome. Si vous voulez contribuer, encore mieux !

Star le repo. Ouvrez des issues. Soumettez des PRs. Partagez-le avec d'autres habitants de bunker.

## Pens√©es finales üí≠

Task-O-Matic va pas r√©volutionner la gestion de projet ou quoi que ce soit de grand. C'est juste un outil que j'ai construit pour m'aider (et vous je l'esp√®re) √† transformer des id√©es chaotiques en t√¢ches organis√©es et ex√©cutables.

C'est pas parfait. C'est pas le code le plus √©l√©gant que j'aie jamais √©crit. Mais √ßa marche. C'est utile. Et c'est le mien.

Si vous coulez dans le chaos de projet et avez besoin d'une approche structur√©e pour d√©couper les choses, essayez-le. Dans le pire des cas, vous aurez un outil CLI cool avec un th√®me bunker.

Dans le meilleur des cas, vous livrerez enfin ce projet auquel vous pensez depuis des mois.

Restez prudents l√†-dehors dans le wasteland, citoyen. Et bon code !

---

{{% goodbye %}}
6:["$","$Le",null,{"tag":"ai","posts":[{"slug":"black-hole-from-ralph-to-eric","title":"From Ralph to Eric","description":"Everyone is going crazy about Ralph loop, and they are cool, but I think we need to upgrade to concept to an Eric loop !","date":"$D2026-01-16T23:01:00.000Z","category":"General","tags":["AI","Ralph lopp","Eric loop"],"content":"$f","readTime":13,"toc":[{"level":2,"title":"The Eric Loop","id":"the-eric-loop"},{"level":2,"title":"But, Why... ?","id":"but-why"},{"level":2,"title":"What would it look like ?","id":"what-would-it-look-like"},{"level":3,"title":"Task-o-matic","id":"task-o-matic"},{"level":3,"title":"Let's get this rolling","id":"lets-get-this-rolling"},{"level":4,"title":"Initialisation and bootstrapping","id":"initialisation-and-bootstrapping"},{"level":4,"title":"Document the requirements of your project","id":"document-the-requirements-of-your-project"},{"level":4,"title":"Petrol is not the only thing that needs refining !","id":"petrol-is-not-the-only-thing-that-needs-refining"},{"level":4,"title":"Touching up and go","id":"touching-up-and-go"},{"level":4,"title":"AGENTS.md","id":"agentsmd"},{"level":4,"title":"Time to let Eric loose...","id":"time-to-let-eric-loose"},{"level":2,"title":"What do you think ? Where do we go from here ?","id":"what-do-you-think-where-do-we-go-from-here"}],"directory":"Black hole/From ralph to Eric","relativePath":"Black hole/From ralph to Eric/index.md","filePath":"/home/didi/workspace/Code/dbuild.io/apps/web/content/blog/Black hole/From ralph to Eric/index.md","isCategory":false},{"slug":"black-hole-passer-de-ralph-a-eric","title":"Passer de Ralph a Eric","description":"Tout le monde ne parle que de la 'Ralph loop', et le principe est cool, mais je pense qu'on peut faire mieux grace au concept de la Boucle a Eric !","date":"$D2026-01-16T23:00:00.000Z","category":"General","tags":["AI","Ralph lopp","Eric loop"],"content":"$10","readTime":14,"toc":[{"level":2,"title":"La boucle Eric","id":"la-boucle-eric"},{"level":2,"title":"Mais, Pourquoi... ?","id":"mais-pourquoi"},{"level":2,"title":"A quoi √ßa ressemblerait ?","id":"a-quoi-a-ressemblerait"},{"level":3,"title":"Task-o-matic","id":"task-o-matic"},{"level":3,"title":"Allons-y","id":"allons-y"},{"level":4,"title":"Initialisation et bootstrap","id":"initialisation-et-bootstrap"},{"level":4,"title":"Documenter les requis de votre projet","id":"documenter-les-requis-de-votre-projet"},{"level":4,"title":"L'essence n'est pas la seule chose qui doit √™tre raffin√©e !","id":"lessence-nest-pas-la-seule-chose-qui-doit-tre-raffine"},{"level":4,"title":"Les petites retouches et c'est parti","id":"les-petites-retouches-et-cest-parti"},{"level":4,"title":"AGENTS.md","id":"agentsmd"},{"level":4,"title":"Et maintenant, on fait rouler Eric...","id":"et-maintenant-on-fait-rouler-eric"},{"level":2,"title":"Vous en pensez quoi ? On va o√π apr√®s ?","id":"vous-en-pensez-quoi-on-va-o-aprs"}],"directory":"Black hole/Passer de Ralph a Eric","relativePath":"Black hole/Passer de Ralph a Eric/index.md","filePath":"/home/didi/workspace/Code/dbuild.io/apps/web/content/blog/Black hole/Passer de Ralph a Eric/index.md","isCategory":false},{"slug":"projects-litechat-litechat-a-local-first-and-self-hostable-ai-chat-app-for-power-user","title":"LiteChat - A local first and self hostable AI chat app for power user","description":"A smallish presentation of my project LiteChat !","date":"$D2025-07-04T10:40:17.000Z","category":"General","tags":["ai","chat","local","server"],"content":"$11","readTime":4,"toc":[],"directory":"Projects/LiteChat/LiteChat : A local first and self hostable AI chat app for power user","relativePath":"Projects/LiteChat/LiteChat : A local first and self hostable AI chat app for power user/index.md","filePath":"/home/didi/workspace/Code/dbuild.io/apps/web/content/blog/Projects/LiteChat/LiteChat : A local first and self hostable AI chat app for power user/index.md","isCategory":false},{"slug":"projects-litechat-litechat-un-chat-ia-local-first-auto-hebergeable-sur-un-server-http","title":"LiteChat - Un chat IA local first, auto hebergeable sur un server HTTP","description":"Une petite pr√©sentation de mon projet LiteChat, en Fran√ßais dans le texte !","date":"$D2025-07-04T10:39:17.000Z","category":"General","tags":["ai","chat","local","server"],"content":"$12","readTime":5,"toc":[],"directory":"Projects/LiteChat/LiteChat : Un chat IA local first, auto hebergeable sur un server HTTP","relativePath":"Projects/LiteChat/LiteChat : Un chat IA local first, auto hebergeable sur un server HTTP/index.md","filePath":"/home/didi/workspace/Code/dbuild.io/apps/web/content/blog/Projects/LiteChat/LiteChat : Un chat IA local first, auto hebergeable sur un server HTTP/index.md","isCategory":false},{"slug":"projects-racebench","title":"RaceBench","description":"Racing LLMs against each other to see who writes the best JavaScript. Spoiler: it's chaos out there!","date":"$D2025-01-17T09:00:00.000Z","category":"General","tags":["benchmark","llm","ai","testing"],"content":"$13","readTime":7,"toc":[{"level":2,"title":"The idea (because every project needs an origin story)","id":"the-idea-because-every-project-needs-an-origin-story"},{"level":2,"title":"What it actually does","id":"what-it-actually-does"},{"level":2,"title":"How it works (the technical bits, minus the boring parts)","id":"how-it-works-the-technical-bits-minus-the-boring-parts"},{"level":3,"title":"The setup","id":"the-setup"},{"level":1,"title":"Deploy to GitHub Pages (or your jungle site of choice)","id":"deploy-to-github-pages-or-your-jungle-site-of-choice"},{"level":1,"title":"I mean, whatever floats your boat","id":"i-mean-whatever-floats-your-boat"},{"level":3,"title":"The test case","id":"the-test-case"},{"level":3,"title":"What we track","id":"what-we-track"},{"level":3,"title":"The UI (because I care about aesthetics, surprisingly)","id":"the-ui-because-i-care-about-aesthetics-surprisingly"},{"level":2,"title":"Versions (because software never ends)","id":"versions-because-software-never-ends"},{"level":3,"title":"v1: The brave beginning","id":"v1-the-brave-beginning"},{"level":3,"title":"v2: The refinement","id":"v2-the-refinement"},{"level":2,"title":"Tech stack (kept it simple, you're welcome)","id":"tech-stack-kept-it-simple-youre-welcome"},{"level":2,"title":"The models (participants in the chaos)","id":"the-models-participants-in-the-chaos"},{"level":2,"title":"See it in action","id":"see-it-in-action"},{"level":2,"title":"What makes this different from other benchmarks?","id":"what-makes-this-different-from-other-benchmarks"},{"level":2,"title":"What I learned (because building stuff teaches you things)","id":"what-i-learned-because-building-stuff-teaches-you-things"},{"level":2,"title":"Future plans (the \"if I get around to it\" section)","id":"future-plans-the-if-i-get-around-to-it-section"},{"level":2,"title":"Wrapping up","id":"wrapping-up"}],"directory":"Projects/RaceBench","relativePath":"Projects/RaceBench/index.md","filePath":"/home/didi/workspace/Code/dbuild.io/apps/web/content/blog/Projects/RaceBench/index.md","isCategory":false},{"slug":"projects-racebench-fr","title":"RaceBench fr","description":"Faire s'affronter des LLMs pour voir qui √©crit le meilleur JavaScript. Spoiler: c'est le chaos complet !","date":"$D2025-01-17T09:00:00.000Z","category":"General","tags":["benchmark","llm","ai","testing","french"],"content":"$14","readTime":8,"toc":[{"level":2,"title":"L'id√©e (parce que tout projet a besoin d'une histoire d'origine)","id":"lide-parce-que-tout-projet-a-besoin-dune-histoire-dorigine"},{"level":2,"title":"Ce que √ßa fait vraiment","id":"ce-que-a-fait-vraiment"},{"level":2,"title":"Comment √ßa marche (les bits techniques, moins les parties ennuyeuses)","id":"comment-a-marche-les-bits-techniques-moins-les-parties-ennuyeuses"},{"level":3,"title":"Le setup","id":"le-setup"},{"level":1,"title":"Deploy to GitHub Pages (or your jungle site of choice)","id":"deploy-to-github-pages-or-your-jungle-site-of-choice"},{"level":1,"title":"I mean, whatever floats your boat","id":"i-mean-whatever-floats-your-boat"},{"level":3,"title":"Le cas de test","id":"le-cas-de-test"},{"level":3,"title":"Ce qu'on tracke","id":"ce-quon-tracke"},{"level":3,"title":"L'UI (parce que je me soucie de l'esth√©tique, √©tonnamment)","id":"lui-parce-que-je-me-soucie-de-lesthtique-tonnamment"},{"level":2,"title":"Versions (parce que le logiciel ne se termine jamais)","id":"versions-parce-que-le-logiciel-ne-se-termine-jamais"},{"level":3,"title":"v1: Le d√©but courageux","id":"v1-le-dbut-courageux"},{"level":3,"title":"v2: Le raffinement","id":"v2-le-raffinement"},{"level":2,"title":"Stack technique (gard√© simple, de rien)","id":"stack-technique-gard-simple-de-rien"},{"level":2,"title":"Les mod√®les (participants dans le chaos)","id":"les-modles-participants-dans-le-chaos"},{"level":2,"title":"Voyez-le en action","id":"voyez-le-en-action"},{"level":2,"title":"Ce qui rend √ßa diff√©rent des autres benchmarks ?","id":"ce-qui-rend-a-diffrent-des-autres-benchmarks"},{"level":2,"title":"Ce que j'ai appris (parce que construire des trucs vous apprend des choses)","id":"ce-que-jai-appris-parce-que-construire-des-trucs-vous-apprend-des-choses"},{"level":2,"title":"Plans futurs (la section \"si je m'y retrouve\")","id":"plans-futurs-la-section-si-je-my-retrouve"},{"level":2,"title":"Conclusion","id":"conclusion"}],"directory":"Projects/RaceBench-fr","relativePath":"Projects/RaceBench-fr/index.md","filePath":"/home/didi/workspace/Code/dbuild.io/apps/web/content/blog/Projects/RaceBench-fr/index.md","isCategory":false},{"slug":"projects-task-o-matic","title":"Task-O-Matic, your AI survival bunker for project chaos","description":"Transform chaotic project ideas into structured, executable task lists through AI assistance. Welcome to the bunker, citizen!","date":"2025-01-17T10:00:00+01:00","category":"General","tags":["ai","task-management","cli","productivity","task-o-matic"],"content":"$15","readTime":6,"toc":[{"level":2,"title":"Welcome to the Bunker üèöÔ∏è","id":"welcome-to-the-bunker"},{"level":2,"title":"What it actually does üìã","id":"what-it-actually-does"},{"level":2,"title":"The tech stack üîß","id":"the-tech-stack"},{"level":2,"title":"Quick start ‚ö°","id":"quick-start"},{"level":2,"title":"Installation üî®","id":"installation"},{"level":2,"title":"Configuration üîë","id":"configuration"},{"level":2,"title":"How it works üöÄ","id":"how-it-works"},{"level":3,"title":"Initialize","id":"initialize"},{"level":3,"title":"Generate a PRD","id":"generate-a-prd"},{"level":3,"title":"Parse into tasks","id":"parse-into-tasks"},{"level":3,"title":"Enhance and plan","id":"enhance-and-plan"},{"level":3,"title":"Execute","id":"execute"},{"level":2,"title":"Multi-AI ü§ñ","id":"multi-ai"},{"level":2,"title":"Benchmarking üìä","id":"benchmarking"},{"level":2,"title":"Local-first storage üíæ","id":"local-first-storage"},{"level":2,"title":"Streaming with reasoning tokens üß†","id":"streaming-with-reasoning-tokens"},{"level":2,"title":"Why the bunker theme? ü§î","id":"why-the-bunker-theme"},{"level":2,"title":"Current status üöß","id":"current-status"},{"level":2,"title":"What's next? üîÆ","id":"whats-next"},{"level":2,"title":"Get involved! ü§ù","id":"get-involved"},{"level":2,"title":"Final thoughts üí≠","id":"final-thoughts"}],"directory":"Projects/Task-O-Matic","relativePath":"Projects/Task-O-Matic/index.md","filePath":"/home/didi/workspace/Code/dbuild.io/apps/web/content/blog/Projects/Task-O-Matic/index.md","isCategory":false},{"slug":"projects-task-o-matic-fr","title":"Task-O-Matic, votre abri de survie IA pour le chaos des projets","description":"Transformez des id√©es de projet chaotiques en listes de t√¢ches structur√©es et ex√©cutables gr√¢ce √† l'IA. Bienvenue dans le bunker, citoyen !","date":"2025-01-17T10:00:00+01:00","category":"General","tags":["ai","task-management","cli","productivity","task-o-matic","french"],"content":"$16","readTime":7,"toc":[{"level":2,"title":"Bienvenue dans le Bunker üèöÔ∏è","id":"bienvenue-dans-le-bunker"},{"level":2,"title":"Ce que √ßa fait vraiment üìã","id":"ce-que-a-fait-vraiment"},{"level":2,"title":"La stack technique üîß","id":"la-stack-technique"},{"level":2,"title":"Installation üî®","id":"installation"},{"level":2,"title":"Configuration üîë","id":"configuration"},{"level":2,"title":"Comment √ßa marche üöÄ","id":"comment-a-marche"},{"level":3,"title":"Initialiser","id":"initialiser"},{"level":3,"title":"G√©n√©rer un PRD","id":"gnrer-un-prd"},{"level":3,"title":"Parser en t√¢ches","id":"parser-en-tches"},{"level":3,"title":"Am√©liorer et planifier","id":"amliorer-et-planifier"},{"level":3,"title":"Ex√©cuter","id":"excuter"},{"level":2,"title":"Multi-IA ü§ñ","id":"multi-ia"},{"level":2,"title":"Benchmarking üìä","id":"benchmarking"},{"level":2,"title":"Stockage local-first üíæ","id":"stockage-local-first"},{"level":2,"title":"Streaming avec tokens de raisonnement üß†","id":"streaming-avec-tokens-de-raisonnement"},{"level":2,"title":"Pourquoi le th√®me bunker ? ü§î","id":"pourquoi-le-thme-bunker"},{"level":2,"title":"Statut actuel üöß","id":"statut-actuel"},{"level":2,"title":"Et apr√®s ? üîÆ","id":"et-aprs"},{"level":2,"title":"Impliquez-vous ! ü§ù","id":"impliquez-vous"},{"level":2,"title":"Pens√©es finales üí≠","id":"penses-finales"}],"directory":"Projects/Task-O-Matic-fr","relativePath":"Projects/Task-O-Matic-fr/index.md","filePath":"/home/didi/workspace/Code/dbuild.io/apps/web/content/blog/Projects/Task-O-Matic-fr/index.md","isCategory":false}]}]
c:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
a:[["$","meta","0",{"charSet":"utf-8"}],["$","title","1",{"children":"Dbuild.dev"}],["$","meta","2",{"name":"description","content":"Dbuild.dev is a portfolio and blog showcasing projects and insights"}],["$","link","3",{"rel":"manifest","href":"/manifest.webmanifest","crossOrigin":"$undefined"}],["$","meta","4",{"name":"robots","content":"index, follow"}],["$","meta","5",{"name":"googlebot","content":"index, follow"}],["$","meta","6",{"property":"og:title","content":"Dbuild.dev"}],["$","meta","7",{"property":"og:description","content":"Dbuild.dev is a portfolio and blog showcasing projects and insights"}],["$","meta","8",{"property":"og:url","content":"https://dbuild.dev/"}],["$","meta","9",{"property":"og:site_name","content":"Dbuild.dev"}],["$","meta","10",{"property":"og:locale","content":"en_US"}],["$","meta","11",{"property":"og:image","content":"https://dbuild.dev/og-image.jpg"}],["$","meta","12",{"property":"og:image:width","content":"1200"}],["$","meta","13",{"property":"og:image:height","content":"630"}],["$","meta","14",{"property":"og:image:alt","content":"Dbuild.dev - Portfolio and Blog"}],["$","meta","15",{"property":"og:type","content":"website"}],["$","meta","16",{"name":"twitter:card","content":"summary_large_image"}],["$","meta","17",{"name":"twitter:title","content":"Dbuild.dev"}],["$","meta","18",{"name":"twitter:description","content":"Dbuild.dev is a portfolio and blog showcasing projects and insights"}],["$","meta","19",{"name":"twitter:image","content":"https://dbuild.dev/og-image.jpg"}],["$","link","20",{"rel":"icon","href":"/favicon.ico","type":"image/x-icon","sizes":"32x32"}]]
8:null

1:"$Sreact.fragment"
2:I[22304,["271","static/chunks/271-6613a09017b67af4.js","442","static/chunks/442-5f28b56b616ff3ca.js","177","static/chunks/app/layout-267ae1d5e4eb94a7.js"],"ThemeProvider"]
3:I[71000,["271","static/chunks/271-6613a09017b67af4.js","442","static/chunks/442-5f28b56b616ff3ca.js","177","static/chunks/app/layout-267ae1d5e4eb94a7.js"],"FloatingNavigation"]
4:I[13842,[],""]
5:I[86880,[],""]
7:I[25803,[],"OutletBoundary"]
9:I[25803,[],"MetadataBoundary"]
b:I[25803,[],"ViewportBoundary"]
d:I[16773,[],""]
:HL["/_next/static/media/e4af272ccee01ff0-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/_next/static/css/9579c992d89962c7.css","style"]
0:{"P":null,"b":"fxCbDsEksO5NPvyXbZAu_","p":"","c":["","blog","tags","ai",""],"i":false,"f":[[["",{"children":["blog",{"children":["tags",{"children":[["tag","ai","d"],{"children":["__PAGE__",{}]}]}]}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/9579c992d89962c7.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","suppressHydrationWarning":true,"children":[["$","head",null,{"children":["$","script",null,{"src":"https://cdn.counter.dev/script.js","data-id":"154c6878-7558-4eff-90f9-bd4904015df1","data-utcoffset":"1","async":true}]}],["$","body",null,{"className":"__className_f367f3","children":["$","$L2",null,{"attribute":"class","defaultTheme":"dark","enableSystem":true,"disableTransitionOnChange":true,"children":[["$","$L3",null,{}],["$","main",null,{"children":["$","$L4",null,{"parallelRouterKey":"children","segmentPath":["children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[],[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]]],"forbidden":"$undefined","unauthorized":"$undefined"}]}]]}]}]]}]]}],{"children":["blog",["$","$1","c",{"children":[null,["$","$L4",null,{"parallelRouterKey":"children","segmentPath":["children","blog","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":["tags",["$","$1","c",{"children":[null,["$","$L4",null,{"parallelRouterKey":"children","segmentPath":["children","blog","children","tags","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["tag","ai","d"],["$","$1","c",{"children":[null,["$","$L4",null,{"parallelRouterKey":"children","segmentPath":["children","blog","children","tags","children","$0:f:0:1:2:children:2:children:2:children:0","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":["__PAGE__",["$","$1","c",{"children":["$L6",null,["$","$L7",null,{"children":"$L8"}]]}],{},null,false]},null,false]},null,false]},null,false]},null,false],["$","$1","h",{"children":[null,["$","$1","4xNDUfVqlG15_Y6bjofOl",{"children":[["$","$L9",null,{"children":"$La"}],["$","$Lb",null,{"children":"$Lc"}],["$","meta",null,{"name":"next-size-adjust","content":""}]]}]]}],false]],"m":"$undefined","G":["$d","$undefined"],"s":false,"S":true}
e:I[73104,["271","static/chunks/271-6613a09017b67af4.js","442","static/chunks/442-5f28b56b616ff3ca.js","792","static/chunks/app/blog/tags/%5Btag%5D/page-aeecd19139ff1a04.js"],"TagDetailContent"]
f:T11ee,
Hello everyone, I‚Äôd like to introduce LiteChat, a project I created.

[Repository](https://github.com/DimitriGilbert/LiteChat) and download link [here](https://github.com/DimitriGilbert/LiteChat/releases) (available in French, English, Spanish, Italian, and German).

LiteChat is an AI chat application I built to interact with both local and remote LLMs‚Äîall within your browser. It‚Äôs designed with a "local-first" philosophy, requiring only an HTTP server to run, with everything else handled in your browser! No tracking, no accounts‚Äîyou bring your own API keys. Data is saved in an IndexedDB database, and you can synchronize your conversations using Git.

Yes, entirely in the browser! üòõ To make this work, I also implemented a virtual file system (again, all in the browser). You can clone repos and include files from the VFS in your conversations! But manually selecting files was tedious, so I integrated tools for managing the VFS and Git.

With the core architecture in place, I added support for HTTP MCP servers. However, standard stdio servers were still missing, so I also built a bridge (rewritten by AI from `mcp-proxy`) to make them work (you can deploy it anywhere, but it‚Äôs not secure!).

Sure, AI is fun, but I was getting tired of text-only interactions. So, I added support for Mermaid diagrams and HTML forms (now you don‚Äôt even need to think about how to phrase your requests!). Sure, Mermaid diagrams aren‚Äôt the prettiest, but since I added a workflow module with visualizations based on [reactflow.dev](https://reactflow.dev), I also included a way for LLMs to generate them! And since plain text isn‚Äôt very engaging, there‚Äôs also a "Beat" block that uses [strudel.cc](http://strudel.cc/) to add auditory feedback.

Testing was getting repetitive, so I created a prompt library with templates‚Äînow you just fill in a form! (Okay, maybe I also needed it for workflows‚Ä¶)

What‚Äôs an Agent, you ask? It‚Äôs a system prompt, tools, and task-specific prompts! So, you also get a library for those.

Prompts and agents can integrate into workflows (that‚Äôs what they‚Äôre designed for!), but you also have transformation steps, user code execution, and custom prompts to facilitate transitions.

As you might have guessed, if I have a way to execute code in workflows, why not run AI-generated code? Yes, you can! In Python or JavaScript. And if you‚Äôre feeling adventurous, you can run JavaScript in "unsafe" mode (`eval` and all‚Äîthat‚Äôs code for "yolo" üòÜ). This can produce cool stuff, like [this one-shot Three.js scroll shooter](https://dimitrigilbert.github.io/racebench/scroller/index.html). You can export it in one click (the template is a bit ugly, but I‚Äôll improve it!).

To avoid losing context, all these UI blocks can be "activated" (or rather, suggested) using rules. Of course, you can add your own rules! There‚Äôs even a button to ask the AI to pick the best ones for your current prompt.

You also get the usual regenerate (with a different model if you like) and forking options. You can even edit responses manually to remove unnecessary parts. Code blocks are also editable with syntax highlighting for common languages (but no autocompletion or fancy features‚Äîlet‚Äôs not push grandma into the bushes!).

To top it all off, you can organize AI "races" with an unlimited number of participants (though that depends on your budget, haha). It‚Äôs great for benchmarking or seeing which model will replace us first. I even built a small tool that takes exported race conversations and turns them into a benchmark site (currently more focused on the JS execution block: [https://dimitrigilbert.github.io/racebench/scroller/index.html](https://dimitrigilbert.github.io/racebench/scroller/index.html) for the "game" mentioned earlier).

I‚Äôm sure I forgot a few things, but you‚Äôve got the gist! üòä

The hosted version is on GitHub Pages, with no tracking and no accounts! You bring your own API keys. You probably won‚Äôt be able to use the hosted version for your local LLM due to HTTPS/HTTP restrictions, but as I said, you can [download it](https://github.com/DimitriGilbert/LiteChat/releases) and host it with a simple HTTP server. There are also localized versions for French, Italian, German, and Spanish. A short (and incomplete) tutorial playlist if you‚Äôre feeling lost: [https://www.youtube.com/playlist?list=PL5Doe56gCsNRdNyfetOYPQw_JkPHO3XVh](https://www.youtube.com/playlist?list=PL5Doe56gCsNRdNyfetOYPQw_JkPHO3XVh)

I hope you enjoy it, and constructive feedback is greatly appreciated! üòä

---
10:T156f,
Salut a tous, pour mon premier post, je vous presente LiteChat dont je sui le cr√©ateur.

[Repo](https://github.com/DimitriGilbert/LiteChat) et ici [pour t√©l√©charger](https://github.com/DimitriGilbert/LiteChat/releases) (en francais, anglais, espagnol, italien et allemand).



C‚Äôest un chat IA que j‚Äôai cr√©√© pour pouvoir utiliser des LLM locaux et distant, le tout dans votre navigateur. Il est con√ßu pour √™tre ‚Äúlocal first‚Äù et n‚Äôa besoin que d‚Äôun serveur HTTP pour fonctionner, tout le reste c‚Äôest dans votre navigateur ! Pas de tracking, pas de compte, vous venez avec vos cl√© d‚ÄôAPI ! Les donn√©es sont sauvegard√©es dans une base de donn√©es IndexeDB et vous pouvez synchroniser vos conversations en utilisant git.

Oui, dans le navigateur :P Pour ce faire, j‚Äôai aussi d√ª impl√©menter un syst√®me de fichiers virtuel (oui, toujour dans le navigateur en utilisant ). Vous avez donc acc√®s aux deux, tant qu‚Äôa faire ! Vous pouvez cloner un repo et joindre des fichiers du VFS dans vos conversations !

Mais comme la s√©lection manuelle des fichiers, c‚Äô√©tait une corv√©e, j‚Äôai int√©gr√© des outils pour le VFS et git !

Ensuite, comme l‚Äôarchitecture de base √©tait l√†, et bien j‚Äôai ajout√© le support des serveurs MCP HTTP, mais il manquait toujours les serveurs stdio ‚Ä¶, donc j‚Äôai aussi ‚Äúfait‚Äù un bridge (r√©√©crit par l‚ÄôIA √† partir de mcp-proxy ) pour les utiliser (vous pouvez le d√©ployer o√π vous voulez mais ce n‚Äôest pas s√©curis√© !)

Apres, c‚Äôest bien mignon l‚ÄôIA, mais j‚Äôen avais un peu marre du texte seul, du coup, j‚Äôai ajout√© le support des diagrammes Mermaid et des formulaires HTML (Comme ca on a meme plus besoin de r√©fl√©chir a quoi lui dire a la machine !). Bon‚Ä¶ apr√®s‚Ä¶les diagrammes Mermaid c‚Äôest un peu moche, et comme j‚Äôai ajout√© un module de workflow avec des visualisations bas√©e sur [https://reactflow.dev/], j‚Äôai aussi ajout√© un moyen pour les LLM de vous en cr√©er ! Et puis comme le text ca fait pas beaucoup de bruit, il y a aussi in block ‚ÄúBeat‚Äù qui utilise (http://strudel.cc/) pour aussi en profiter de maniere auditive !

Et puis bon, en testant, toujours taper les m√™mes prompts avec juste quelques diff√©rences c‚Äô√©tait lourd aussi, j‚Äôai donc fait un module de biblioth√®que de prompts avec des mod√®les pour qu‚Äôy ai plus qu‚Äôa remplir un formulaire ;) (peut etre aussi que j‚Äôen avais besoin pour les workflows‚Ä¶)

‚ÄòPis, un Agents, c‚Äôest quoi ? hein ? bah un prompt syst√®me, des outils et des prompts sp√©cifiques pour les t√¢ches ! Donc √ßa aussi vous en avez une librairie !

Les prompts et les agents peuvent s‚Äôint√©grer dans les workflows (duh, ils √©taient faits pour √ßa !) mais vous avez aussi des √©tapes de ‚Äútransformation‚Äù/ex√©cution de code utilisateur/‚Äúprompt personnalis√©‚Äù pour faciliter le transit !

Comme vous l‚Äôavez peut-√™tre devin√©, si j‚Äôai une forme d‚Äôex√©cution de code pour les workflows, Est ce que je pourrais t‚Äôy donc pas faire tourner le code g√©n√©r√© par l‚ÄôIA, hein ? Eeeeh bha SI ! En Python ou JavaScript Et m√™me que si vous √™te un dinguo, vous pouvez faire tourner le js en mode ‚Äúunsafe‚Äù (eval et yolo XD) du coup ca peut produire des trucs (comme ce one shot threejs scroll shooter ) que vous pouvez exporter en 1 clic (le template est moche mais je vais y travailler !)

Afin de n‚Äôpas completement obliterer le context, tous ces jolis blocs d‚ÄôUI peuvent √™tre ‚Äúactiv√©s‚Äù (plut√¥t sugg√©r√©s ^^) en utilisant des r√®gles. Evidement, vous pouvez ajouter vos r√®gles a vous ! Et meme que vous avez un bouton pour demander a l‚ÄôIA de les choisir pour vous pour votre prompt actuel !

Bien s√ªr, vous avez les habituels regen (avec un mod√®le diff√©rent si le coeur vous en dis) et forking, mais vous pouvez aussi modifier une r√©ponse a la main pour d√©gager l‚Äôinutile. D‚Äôailleur, Les blocs de code sont aussi modifiable avec la coloration syntaxique pour les langages les plus courants, (mais pas d‚Äôauto-compl√©tion ou autre truc de bogoss, on va pas poussez m√©m√© dans les orties !).

Pour couronner le tout, vous pouvez organiser des courses d‚ÄôIA avec un nombre illimit√© de participants (ca dependra de la profondeur de votre portefeuille :P). C‚Äôest cool pour faire des benchmarks ou quand on veut voir laquelle des machines prendra notre place en premier‚Ä¶ J‚Äôai m√™me fait un petit outil qui prend une conversation de course export√©e et qui cr√©e un mini site de benchmark (plus cibl√© sur le bloc d‚Äôex√©cution JS pour l‚Äôinstant https://dimitrigilbert.github.io/racebench/scroller/index.html pour le ‚Äújeu‚Äù d‚Äôavant)

J‚Äôoublie certainement quelques bricoles, mais vous avez compris l‚Äôessentiel ^^

La version h√©berg√©e est sur les pages GitHub et il n‚Äôy a pas de tracking, pas de compte ! Vous apportez vos propres cl√©s API ! Vous ne pourrez probablement pas utiliser la version h√©berg√©e pour votre LLM local √† cause des restrictions https/http, mais comme je l‚Äôai dit, vous pouvez t√©l√©charger https://github.com/DimitriGilbert/LiteChat/releases et h√©berger avec un simple serveur HTTP. Vous avez m√™me des versions localis√©es pour le fran√ßais, l‚Äôitalien, l‚Äôallemand et l‚Äôespagnol. Une petite playlist (tr√®s incompl√®te) de tutoriels si vous vous sentez un peu perdu https://www.youtube.com/playlist?list=PL5Doe56gCsNRdNyfetOYPQw_JkPHO3XVh

J‚Äôesp√®re que vous appr√©cierez et les commentaires constructifs sont grandement appr√©ci√©s :D11:T1e05,
So, you know how everyone's benchmarking LLMs these days? Like, "oh look, this model scored 87.3% on this synthetic test suite" or whatever. Boring, right? I mean, who cares if a model can reverse a string perfectly when you'll never ask it to reverse a string in real life? üòõ

That's why I built **RaceBench** ‚Äî a benchmarking tool that actually tests something useful: how well different LLMs can write real, working JavaScript code. Not some "hello world" nonsense, but actual creative coding tasks ‚Äî games, visualisations, interactive stuff you might actually want to build.

## The idea (because every project needs an origin story)

One day I was curious: if I ask different AI models to write the same game, which one would actually produce playable code? Not "technically correct but crashes on the third frame" code, but stuff you could run and enjoy.

So I created a benchmark where I ask models to build a 2D scroll shooter game using Three.js. Not exactly "easy mode" for an AI ‚Äî it requires understanding 3D graphics, game loops, user input, and making everything work together without blowing up the browser. Spoiler: some models handle it beautifully, others... well, let's say it's educational to watch them try üòä

## What it actually does

RaceBench is a static HTML dashboard that displays results from testing various LLMs on JavaScript code generation. Here's the cool part:

- **Real-world benchmarking** ‚Äî We test creative coding tasks, not synthetic "reverse this string" nonsense
- **Executable results** ‚Äî Every model's output can be run directly. See the code, click preview, judge for yourself
- **Multi-dimensional analysis** ‚Äî It's not just "did it work?" We track prompt tokens, completion tokens, TTFT (time to first token), generation time, and success rate
- **Educational value** ‚Äî Compare how different models structure their code, their approaches to the same problem, and learn from the variety (even the bad ones!)

## How it works (the technical bits, minus the boring parts)

### The setup

Everything's static ‚Äî pure HTML/CSS/JS with no backend. Deploy it anywhere. GitHub Pages? Sure. Netlify? Why not. Your Raspberry Pi at home? Absolutely. The whole thing weighs about as much as a small hamster üêπ

```bash
# Clone it
git clone https://github.com/DimitriGilbert/racebench.git

# Deploy to GitHub Pages (or your jungle site of choice)
# I mean, whatever floats your boat
```

### The test case

The benchmark uses a 2D scroll shooter game built with Three.js as the test scenario. Each model gets the same prompt and has to produce working code. We extract the last ````runjs` code block from each response (because models love to chat before giving you the goods, you know?).

### What we track

For each model, RaceBench captures:

- **Prompt tokens** ‚Äî How much it needed to understand what we wanted
- **Completion tokens** ‚Äî How verbose it got with the solution
- **TTFT** (Time To First Token) ‚Äî Responsiveness metrics
- **Generation time** ‚Äî Total time to produce the code
- **Success rate** ‚Äî Did it actually run? Or did it explode?

### The UI (because I care about aesthetics, surprisingly)

It's got this nice dark mode UI that doesn't look like it was designed by a sysadmin in 2003. Two view modes:

- **Card grid** ‚Äî Pretty cards for each model with key metrics at a glance
- **Sortable table** ‚Äî For when you want to data-nerd out and sort by TTFT or token count

Click any model and you get:
- Live preview (opens in a new tab, runs the actual code)
- Raw response view (see exactly what the model said)
- All the metrics broken down

## Versions (because software never ends)

### v1: The brave beginning

Tested 18 models. Learned a lot. Realised some models are impressively creative while others are... trying their best. Let's put it that way. üòõ

### v2: The refinement

Narrowed it down to 10 models. Better metrics extraction. Cleaner UI. Still watching models struggle with Three.js in entertaining ways.

## Tech stack (kept it simple, you're welcome)

- **Node.js** ‚Äî Because JavaScript is life
- **Tailwind CSS (via CDN)** ‚Äî Styling without the build step headaches
- **Simple-Datatables** ‚Äî For when you want sortable tables but don't want to write them yourself
- **Three.js (dynamic)** ‚Äî Loaded on demand for the live previews

Minimal dependencies. No complex build pipelines. Just works. I like that philosophy.

## The models (participants in the chaos)

We've tested a bunch of the big names:

- Claude Sonnet 4
- DeepSeek R1
- GPT-4.1
- Gemini 2.5
- Grok 4
- Kimi K2
- Qwen3
- ...and more

Each one brings its own personality to the code. Some are terse and efficient. Others are verbose and over-engineered. A few produce code that makes you question everything you know about software development. All educational in their own way üòä

## See it in action

Don't just take my word for it ‚Äî go play with the actual dashboard:

**[Live Demo](https://dimitrigilbert.github.io/racebench/scroller/index.html)**

Click around. Run some code. See which model you'd want on your team. Spoiler: the answer might surprise you (or it might be exactly what you expect. LLMs are weird like that).

## What makes this different from other benchmarks?

I'm glad you asked (I know you didn't, but pretend you did) üòõ

1. **Real tasks, not toy problems** ‚Äî We test actual creative coding, not "sort this array" nonsense
2. **You can run the results** ‚Äî Not just a score ‚Äî actual working code you can inspect and execute
3. **No server needed** ‚Äî Static HTML means you can host it anywhere or even run it locally
4. **Transparent metrics** ‚Äî See exactly what we measured and how
5. **Educational** ‚Äî Compare approaches, learn patterns, see what works (and what definitely doesn't)

## What I learned (because building stuff teaches you things)

- Some models are surprisingly good at 3D graphics code
- Others clearly haven't seen many Three.js examples in their training
- TTFT doesn't always correlate with code quality
- Verbose models aren't necessarily better ‚Äî sometimes they're just... chatty
- The best model depends on what you value: speed, efficiency, code clarity, or just getting something that runs

## Future plans (the "if I get around to it" section)

I'd like to:

- Add more test cases (different types of creative coding challenges)
- Test more models (new ones keep popping up like mushrooms)
- Improve the metrics (maybe add code quality scoring?)
- Add a "leaderboard" view for those who love rankings
- Maybe, possibly, contribute the data to some broader benchmark effort (if such a thing exists and wants this kind of real-world data)

But let's be honest ‚Äî I'll probably add a new test case before I remember to do any of that. Priorities, right? üòä

## Wrapping up

RaceBench isn't trying to be the definitive AI benchmark. It's just me trying to answer a simple question: "Which AI writes the best JavaScript code for real projects?" And having fun seeing them all try.

If you find it useful, great. If you learn something from the different code styles, even better. If you just enjoy clicking through live previews of AI-generated games and thinking "wow, that actually works," then I've done my job.

Feel free to fork it, add your own models, test different prompts. The more data, the better. Just don't ask me which model is "best" ‚Äî that depends on what you're building, your timeline, your budget, and probably the phase of the moon. LLMs are like that üòõ

**GitHub:** [DimitriGilbert/racebench](https://github.com/DimitriGilbert/racebench)

Now go race some models. May the best code win.

{{% goodbye %}}
12:T2777,
So, who here has **project ideas** ? And among these ideas, some turn into... uh, actual tasks, right ?

Ok ok, so, get me a structured plan, broken down tasks, and anything resembling order, like, right NOW !

Yeah...it's ok, we've all been there üòõ. The wasteland of unorganized thoughts is a dangerous place. But worry not, I built us a bunker !

## Welcome to the Bunker üèöÔ∏è

[Task-O-Matic](https://github.com/DimitriGilbert/task-o-matic) is an AI-powered task management system styled with a post-apocalyptic survival bunker theme (because why not ?). It transforms your chaotic project ideas into structured, executable task lists through AI assistance.

Think of it as your personal task commander in the aftermath of project anarchy.

## The Mission (What it does) üìã

Task-O-Matic is not state of the art ‚Äî I'm not a bloody AI researcher ‚Äî but it *does* work and it's rather useful for us bunker dwellers :

- **Idea to PRD generation** : Take your half-baked idea and let AI turn it into a proper Product Requirements Document
- **PRD parsing into structured tasks** : Break down that PRD into actual, doable tasks
- **AI-powered task enhancement** : Let the AI improve, split, and plan your tasks
- **Multi-AI generation** : Run multiple AI models in parallel and compare results (battle royale style)
- **Task execution with external tools** : Connect to Opencode, Claude Code, Gemini CLI, and more
- **Benchmarking system** : Test different AI models with git worktree isolation
- **Existing project adoption** : Attach Task-O-Matic to existing projects (`init attach`)
- **Local-first storage** : Everything lives in your `.task-o-matic` directory (the bunker's archives)
- **Hierarchical task breakdown** : Subtasks, sub-subtasks, sub-sub-sub... you get the idea
- **Streaming responses** : Watch the AI think with reasoning tokens

## The Tech Stack üîß

Built with a collection of reliable tools that survived the apocalypse:

- **TypeScript** : Because type safety is survival safety
- **Bun** : Fast package manager (needed when running from generators)
- **Vercel AI SDK v6** : The magic behind the AI curtain
- **Commander.js** : CLI parsing (I know a thing or two about CLI tools...)
- **Inquirer.js** : Interactive prompts (for when you need to make decisions in the bunker)
- **Zod** : Schema validation (don't want corrupted data in the archives)

## TLDR (Quick Start) ‚ö°

Oh, I know you're impatient (me too...), so here is a quick run down to get going !

```bash
# Clone the repo
git clone https://github.com/DimitriGilbert/task-o-matic.git
cd task-o-matic

# Install dependencies
bun install

# Link for local development (optional but recommended)
bun link

# Initialize a new project in your existing directory
task-o-matic init attach

# Generate a PRD from your idea
task-o-matic prd generate "Build a survival bunker dashboard"

# Parse the PRD into tasks
task-o-matic prd parse

# Let the AI enhance your tasks
task-o-matic task enhance --all

# Execute tasks with your AI of choice
task-o-matic task execute --ai claude
```

## Installation üî®

First things first, let's get you installed in the bunker !

```bash
# Clone the repository
git clone https://github.com/DimitriGilbert/task-o-matic.git
cd task-o-matic

# Install dependencies with Bun
bun install

# Make the CLI available globally (optional but handy)
bun link
```

You can also install via npm if you prefer the old ways:

```bash
npm install -g @dimitrigilbert/task-o-matic
```

## Configuration üîë

Task-O-Matic needs to know how to reach the outside world (AI providers). Set your API keys as environment variables:

```bash
# OpenAI
export OPENAI_API_KEY="your-openai-api-key"

# Anthropic (Claude)
export ANTHROPIC_API_KEY="your-anthropic-api-key"

# Google (Gemini)
export GOOGLE_API_KEY="your-google-api-key"

# OpenRouter (if you want access to many models)
export OPENROUTER_API_KEY="your-openrouter-api-key"
```

I'd recommend putting these in your `.env` file or shell profile. Don't commit them to git, obviously.

## Workflow Example üöÄ

Let's walk through a typical bunker workflow, shall we ?

### Step 1: Initialize Your Bunker

```bash
# Attach to an existing project directory
task-o-matic init attach

# Or create a new project
task-o-matic init new my-survival-project
```

This creates a `.task-o-matic` directory where all your data lives. It's like your bunker's storage room.

### Step 2: Generate a PRD from Your Idea

```bash
task-o-matic prd generate "I want to build a personal finance tracker that works offline, syncs when online, and has a dark mode because the bunker lights are harsh"
```

The AI will interview you (via inquirer prompts) to clarify requirements and generate a proper PRD document.

### Step 3: Parse the PRD into Tasks

```bash
task-o-matic prd parse
```

This breaks down your PRD into a hierarchical task structure. You'll get tasks, subtasks, and sub-subtasks organized by priority and dependencies.

### Step 4: Enhance and Plan Your Tasks

```bash
# Let AI improve all tasks
task-o-matic task enhance --all

# Plan execution order based on dependencies
task-o-matic task plan

# Split a large task into smaller chunks
task-o-matic task split --id task-123
```

### Step 5: Execute Tasks with AI

```bash
# Execute a specific task with Claude
task-o-matic task execute --id task-123 --ai claude

# Execute all pending tasks with multiple AI models
task-o-matic task execute --all --ai claude,gemini,gpt4

# Execute with external tools
task-o-matic task execute --id task-123 --tool opencode
```

The AI will read the task description, context, and generate the actual code or documentation. You can review before committing.

## Multi-AI Generation ü§ñ

One of the cooler features (in my humble opinion) is the ability to run multiple AI models in parallel:

```bash
# Generate solutions from multiple providers
task-o-matic task generate-multi \
  --id task-123 \
  --providers claude,gpt4,gemini \
  --compare

# This will show you a comparison of the different approaches
```

Great for when you want to see different perspectives or verify the best solution. It's like having a council of AI advisors in your bunker üòä

## Benchmarking System üìä

If you're curious about which AI model performs best for your use cases:

```bash
# Run benchmarks with git worktree isolation
task-o-matic benchmark run \
  --models claude,gpt4,gemini \
  --task-set my-tasks

# View results
task-o-matic benchmark results

# Compare specific runs
task-o-matic benchmark compare --run-id run1,run2
```

This uses git worktrees to ensure clean environments for each benchmark run. No contamination !

## Existing Project Adoption üè†

Already have a project you want to bring into the bunker ?

```bash
# Navigate to your project directory
cd my-existing-project

# Attach Task-O-Matic
task-o-matic init attach

# It will scan your codebase and create initial context
```

Task-O-Matic will analyze your existing code structure and use it as context for future task generation.

## Local-First Storage üíæ

Everything Task-O-Matic creates lives in your `.task-o-matic` directory:

```
.task-o-matic/
‚îú‚îÄ‚îÄ prds/           # Product Requirements Documents
‚îú‚îÄ‚îÄ tasks/          # Task definitions and status
‚îú‚îÄ‚îÄ history/        # Execution history
‚îú‚îÄ‚îÄ benchmarks/     # Benchmark results
‚îî‚îÄ‚îÄ config.json     # Your configuration
```

No cloud dependency, no data leaving your bunker (unless you choose to). Your data, your rules.

## Streaming with Reasoning Tokens üß†

When executing tasks, you can watch the AI think in real-time:

```bash
task-o-matic task execute --id task-123 --stream
```

This shows you the reasoning tokens as they're generated. It's fascinating to see how the AI approaches problems (sometimes in ways I wouldn't expect, which is good).

## Why the Bunker Theme ? ü§î

I know, I know, it's a bit weird. But here's the thing:

1. It's fun üòõ
2. Projects can feel like chaotic wastelands
3. Having a structured system feels like building a safe shelter
4. "Bunker" is a great metaphor for local-first, isolated development
5. Why not ?

Also, calling users "citizens" and projects "missions" makes me chuckle. If it makes you smile too, then it's worth it.

## Current Status üöß

Task-O-Matic is in active development at **v0.1.5-beta.1**. Things might break, features might change, but the core functionality works.

If you find bugs (and I'm sure there are nests of them), please open an issue on the [GitHub repo](https://github.com/DimitriGilbert/task-o-matic/issues). I'm the only user right now, so finding bugs is becoming... challenging üòä

## What's Next ? üîÆ

Here's what I'm planning (subject to change based on what the wasteland throws at us):

- [ ] More AI provider integrations
- [ ] Web UI (because some people prefer graphical interfaces)
- [ ] Team collaboration features (multi-bunker coordination)
- [ ] Plugin system for custom tools
- [ ] Better dependency visualization
- [ ] Task templates for common project types
- [ ] Integration with CI/CD pipelines

## Get Involved ! ü§ù

This is a side project I built because I needed it. If you find it useful, that's awesome. If you want to contribute, even better !

- Star the repo if you like it
- Open issues for bugs or feature requests
- Submit PRs if you want to add something cool
- Share it with other bunker dwellers

## Final Thoughts üí≠

Task-O-Matic isn't going to revolutionize project management or anything grand like that. It's just a tool I built to help me (and hopefully you) turn chaotic ideas into organized, executable tasks.

It's not perfect. It's not the most elegant code I've ever written. But it works, it's useful, and it's mine.

If you're drowning in project chaos and need a structured approach to breaking things down, give it a shot. Worst case, you'll have a cool CLI tool with a bunker theme.

Best case, you'll actually ship that project you've been thinking about for months.

Stay safe out there in the wasteland, citizen. And happy coding !

---

{{% goodbye %}}
6:["$","$Le",null,{"tag":"ai","posts":[{"slug":"projects-litechat-litechat-a-local-first-and-self-hostable-ai-chat-app-for-power-user","title":"LiteChat - A local first and self hostable AI chat app for power user","description":"A smallish presentation of my project LiteChat !","date":"$D2025-07-04T10:40:17.000Z","category":"General","tags":["ai","chat","local","server"],"content":"$f","readTime":4,"toc":[],"directory":"Projects/LiteChat/LiteChat : A local first and self hostable AI chat app for power user","relativePath":"Projects/LiteChat/LiteChat : A local first and self hostable AI chat app for power user/index.md","filePath":"/home/didi/workspace/Code/dbuild.io/apps/web/content/blog/Projects/LiteChat/LiteChat : A local first and self hostable AI chat app for power user/index.md","isCategory":false},{"slug":"projects-litechat-litechat-un-chat-ia-local-first-auto-hebergeable-sur-un-server-http","title":"LiteChat - Un chat IA local first, auto hebergeable sur un server HTTP","description":"Une petite pr√©sentation de mon projet LiteChat, en Fran√ßais dans le texte !","date":"$D2025-07-04T10:39:17.000Z","category":"General","tags":["ai","chat","local","server"],"content":"$10","readTime":5,"toc":[],"directory":"Projects/LiteChat/LiteChat : Un chat IA local first, auto hebergeable sur un server HTTP","relativePath":"Projects/LiteChat/LiteChat : Un chat IA local first, auto hebergeable sur un server HTTP/index.md","filePath":"/home/didi/workspace/Code/dbuild.io/apps/web/content/blog/Projects/LiteChat/LiteChat : Un chat IA local first, auto hebergeable sur un server HTTP/index.md","isCategory":false},{"slug":"projects-racebench","title":"RaceBench","description":"Racing LLMs against each other to see who writes the best JavaScript. Spoiler: it's chaos out there!","date":"$D2025-01-17T09:00:00.000Z","category":"General","tags":["benchmark","llm","ai","testing"],"content":"$11","readTime":7,"toc":[{"level":2,"title":"The idea (because every project needs an origin story)","id":"the-idea-because-every-project-needs-an-origin-story"},{"level":2,"title":"What it actually does","id":"what-it-actually-does"},{"level":2,"title":"How it works (the technical bits, minus the boring parts)","id":"how-it-works-the-technical-bits-minus-the-boring-parts"},{"level":3,"title":"The setup","id":"the-setup"},{"level":1,"title":"Clone it","id":"clone-it"},{"level":1,"title":"Deploy to GitHub Pages (or your jungle site of choice)","id":"deploy-to-github-pages-or-your-jungle-site-of-choice"},{"level":1,"title":"I mean, whatever floats your boat","id":"i-mean-whatever-floats-your-boat"},{"level":3,"title":"The test case","id":"the-test-case"},{"level":3,"title":"What we track","id":"what-we-track"},{"level":3,"title":"The UI (because I care about aesthetics, surprisingly)","id":"the-ui-because-i-care-about-aesthetics-surprisingly"},{"level":2,"title":"Versions (because software never ends)","id":"versions-because-software-never-ends"},{"level":3,"title":"v1: The brave beginning","id":"v1-the-brave-beginning"},{"level":3,"title":"v2: The refinement","id":"v2-the-refinement"},{"level":2,"title":"Tech stack (kept it simple, you're welcome)","id":"tech-stack-kept-it-simple-youre-welcome"},{"level":2,"title":"The models (participants in the chaos)","id":"the-models-participants-in-the-chaos"},{"level":2,"title":"See it in action","id":"see-it-in-action"},{"level":2,"title":"What makes this different from other benchmarks?","id":"what-makes-this-different-from-other-benchmarks"},{"level":2,"title":"What I learned (because building stuff teaches you things)","id":"what-i-learned-because-building-stuff-teaches-you-things"},{"level":2,"title":"Future plans (the \"if I get around to it\" section)","id":"future-plans-the-if-i-get-around-to-it-section"},{"level":2,"title":"Wrapping up","id":"wrapping-up"}],"directory":"Projects/RaceBench","relativePath":"Projects/RaceBench/index.md","filePath":"/home/didi/workspace/Code/dbuild.io/apps/web/content/blog/Projects/RaceBench/index.md","isCategory":false},{"slug":"projects-task-o-matic","title":"Task-O-Matic, your AI survival bunker for project chaos","description":"Transform chaotic project ideas into structured, executable task lists through AI assistance. Welcome to the bunker, citizen!","date":"2025-01-17T10:00:00+01:00","category":"General","tags":["ai","task-management","cli","productivity","task-o-matic"],"content":"$12","readTime":8,"toc":[{"level":2,"title":"Welcome to the Bunker üèöÔ∏è","id":"welcome-to-the-bunker"},{"level":2,"title":"The Mission (What it does) üìã","id":"the-mission-what-it-does"},{"level":2,"title":"The Tech Stack üîß","id":"the-tech-stack"},{"level":2,"title":"TLDR (Quick Start) ‚ö°","id":"tldr-quick-start"},{"level":1,"title":"Clone the repo","id":"clone-the-repo"},{"level":1,"title":"Install dependencies","id":"install-dependencies"},{"level":1,"title":"Link for local development (optional but recommended)","id":"link-for-local-development-optional-but-recommended"},{"level":1,"title":"Initialize a new project in your existing directory","id":"initialize-a-new-project-in-your-existing-directory"},{"level":1,"title":"Generate a PRD from your idea","id":"generate-a-prd-from-your-idea"},{"level":1,"title":"Parse the PRD into tasks","id":"parse-the-prd-into-tasks"},{"level":1,"title":"Let the AI enhance your tasks","id":"let-the-ai-enhance-your-tasks"},{"level":1,"title":"Execute tasks with your AI of choice","id":"execute-tasks-with-your-ai-of-choice"},{"level":2,"title":"Installation üî®","id":"installation"},{"level":1,"title":"Clone the repository","id":"clone-the-repository"},{"level":1,"title":"Install dependencies with Bun","id":"install-dependencies-with-bun"},{"level":1,"title":"Make the CLI available globally (optional but handy)","id":"make-the-cli-available-globally-optional-but-handy"},{"level":2,"title":"Configuration üîë","id":"configuration"},{"level":1,"title":"OpenAI","id":"openai"},{"level":1,"title":"Anthropic (Claude)","id":"anthropic-claude"},{"level":1,"title":"Google (Gemini)","id":"google-gemini"},{"level":1,"title":"OpenRouter (if you want access to many models)","id":"openrouter-if-you-want-access-to-many-models"},{"level":2,"title":"Workflow Example üöÄ","id":"workflow-example"},{"level":3,"title":"Step 1: Initialize Your Bunker","id":"step-1-initialize-your-bunker"},{"level":1,"title":"Attach to an existing project directory","id":"attach-to-an-existing-project-directory"},{"level":1,"title":"Or create a new project","id":"or-create-a-new-project"},{"level":3,"title":"Step 2: Generate a PRD from Your Idea","id":"step-2-generate-a-prd-from-your-idea"},{"level":3,"title":"Step 3: Parse the PRD into Tasks","id":"step-3-parse-the-prd-into-tasks"},{"level":3,"title":"Step 4: Enhance and Plan Your Tasks","id":"step-4-enhance-and-plan-your-tasks"},{"level":1,"title":"Let AI improve all tasks","id":"let-ai-improve-all-tasks"},{"level":1,"title":"Plan execution order based on dependencies","id":"plan-execution-order-based-on-dependencies"},{"level":1,"title":"Split a large task into smaller chunks","id":"split-a-large-task-into-smaller-chunks"},{"level":3,"title":"Step 5: Execute Tasks with AI","id":"step-5-execute-tasks-with-ai"},{"level":1,"title":"Execute a specific task with Claude","id":"execute-a-specific-task-with-claude"},{"level":1,"title":"Execute all pending tasks with multiple AI models","id":"execute-all-pending-tasks-with-multiple-ai-models"},{"level":1,"title":"Execute with external tools","id":"execute-with-external-tools"},{"level":2,"title":"Multi-AI Generation ü§ñ","id":"multi-ai-generation"},{"level":1,"title":"Generate solutions from multiple providers","id":"generate-solutions-from-multiple-providers"},{"level":1,"title":"This will show you a comparison of the different approaches","id":"this-will-show-you-a-comparison-of-the-different-approaches"},{"level":2,"title":"Benchmarking System üìä","id":"benchmarking-system"},{"level":1,"title":"Run benchmarks with git worktree isolation","id":"run-benchmarks-with-git-worktree-isolation"},{"level":1,"title":"View results","id":"view-results"},{"level":1,"title":"Compare specific runs","id":"compare-specific-runs"},{"level":2,"title":"Existing Project Adoption üè†","id":"existing-project-adoption"},{"level":1,"title":"Navigate to your project directory","id":"navigate-to-your-project-directory"},{"level":1,"title":"Attach Task-O-Matic","id":"attach-task-o-matic"},{"level":1,"title":"It will scan your codebase and create initial context","id":"it-will-scan-your-codebase-and-create-initial-context"},{"level":2,"title":"Local-First Storage üíæ","id":"local-first-storage"},{"level":2,"title":"Streaming with Reasoning Tokens üß†","id":"streaming-with-reasoning-tokens"},{"level":2,"title":"Why the Bunker Theme ? ü§î","id":"why-the-bunker-theme"},{"level":2,"title":"Current Status üöß","id":"current-status"},{"level":2,"title":"What's Next ? üîÆ","id":"whats-next"},{"level":2,"title":"Get Involved ! ü§ù","id":"get-involved"},{"level":2,"title":"Final Thoughts üí≠","id":"final-thoughts"}],"directory":"Projects/Task-O-Matic","relativePath":"Projects/Task-O-Matic/index.md","filePath":"/home/didi/workspace/Code/dbuild.io/apps/web/content/blog/Projects/Task-O-Matic/index.md","isCategory":false}]}]
c:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
a:[["$","meta","0",{"charSet":"utf-8"}],["$","title","1",{"children":"Dbuild.dev"}],["$","meta","2",{"name":"description","content":"Dbuild.dev is a portfolio and blog showcasing projects and insights"}],["$","link","3",{"rel":"manifest","href":"/manifest.webmanifest","crossOrigin":"$undefined"}],["$","meta","4",{"name":"robots","content":"index, follow"}],["$","meta","5",{"name":"googlebot","content":"index, follow"}],["$","meta","6",{"property":"og:title","content":"Dbuild.dev"}],["$","meta","7",{"property":"og:description","content":"Dbuild.dev is a portfolio and blog showcasing projects and insights"}],["$","meta","8",{"property":"og:url","content":"https://dbuild.dev/"}],["$","meta","9",{"property":"og:site_name","content":"Dbuild.dev"}],["$","meta","10",{"property":"og:locale","content":"en_US"}],["$","meta","11",{"property":"og:image","content":"https://dbuild.dev/og-image.jpg"}],["$","meta","12",{"property":"og:image:width","content":"1200"}],["$","meta","13",{"property":"og:image:height","content":"630"}],["$","meta","14",{"property":"og:image:alt","content":"Dbuild.dev - Portfolio and Blog"}],["$","meta","15",{"property":"og:type","content":"website"}],["$","meta","16",{"name":"twitter:card","content":"summary_large_image"}],["$","meta","17",{"name":"twitter:title","content":"Dbuild.dev"}],["$","meta","18",{"name":"twitter:description","content":"Dbuild.dev is a portfolio and blog showcasing projects and insights"}],["$","meta","19",{"name":"twitter:image","content":"https://dbuild.dev/og-image.jpg"}],["$","link","20",{"rel":"icon","href":"/favicon.ico","type":"image/x-icon","sizes":"32x32"}]]
8:null

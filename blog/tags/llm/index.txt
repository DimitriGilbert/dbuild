1:"$Sreact.fragment"
2:I[22304,["239","static/chunks/239-c47e640cea1e8478.js","802","static/chunks/802-cb271f7d125031e5.js","177","static/chunks/app/layout-8f6931abe4e72e4f.js"],"ThemeProvider"]
3:I[71000,["239","static/chunks/239-c47e640cea1e8478.js","802","static/chunks/802-cb271f7d125031e5.js","177","static/chunks/app/layout-8f6931abe4e72e4f.js"],"FloatingNavigation"]
4:I[13842,[],""]
5:I[86880,[],""]
7:I[25803,[],"OutletBoundary"]
9:I[25803,[],"MetadataBoundary"]
b:I[25803,[],"ViewportBoundary"]
d:I[16773,[],""]
:HL["/_next/static/media/e4af272ccee01ff0-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/_next/static/css/50152c3edddaf6a5.css","style"]
0:{"P":null,"b":"v4LhJB9LIBQDYn6TLS-xY","p":"","c":["","blog","tags","llm",""],"i":false,"f":[[["",{"children":["blog",{"children":["tags",{"children":[["tag","llm","d"],{"children":["__PAGE__",{}]}]}]}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/50152c3edddaf6a5.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","suppressHydrationWarning":true,"children":[["$","head",null,{"children":["$","script",null,{"src":"https://cdn.counter.dev/script.js","data-id":"154c6878-7558-4eff-90f9-bd4904015df1","data-utcoffset":"1","async":true}]}],["$","body",null,{"className":"__className_f367f3","children":["$","$L2",null,{"attribute":"class","defaultTheme":"dark","enableSystem":true,"disableTransitionOnChange":true,"children":[["$","$L3",null,{}],["$","main",null,{"children":["$","$L4",null,{"parallelRouterKey":"children","segmentPath":["children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[],[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]]],"forbidden":"$undefined","unauthorized":"$undefined"}]}]]}]}]]}]]}],{"children":["blog",["$","$1","c",{"children":[null,["$","$L4",null,{"parallelRouterKey":"children","segmentPath":["children","blog","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":["tags",["$","$1","c",{"children":[null,["$","$L4",null,{"parallelRouterKey":"children","segmentPath":["children","blog","children","tags","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["tag","llm","d"],["$","$1","c",{"children":[null,["$","$L4",null,{"parallelRouterKey":"children","segmentPath":["children","blog","children","tags","children","$0:f:0:1:2:children:2:children:2:children:0","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":["__PAGE__",["$","$1","c",{"children":["$L6",null,["$","$L7",null,{"children":"$L8"}]]}],{},null,false]},null,false]},null,false]},null,false]},null,false],["$","$1","h",{"children":[null,["$","$1","nVBipv6qmBbwe9XGbhTwG",{"children":[["$","$L9",null,{"children":"$La"}],["$","$Lb",null,{"children":"$Lc"}],["$","meta",null,{"name":"next-size-adjust","content":""}]]}]]}],false]],"m":"$undefined","G":["$d","$undefined"],"s":false,"S":true}
e:I[73104,["239","static/chunks/239-c47e640cea1e8478.js","802","static/chunks/802-cb271f7d125031e5.js","792","static/chunks/app/blog/tags/%5Btag%5D/page-af6646a5e411dccb.js"],"TagDetailContent"]
f:T1e1b,
_This content has been generated by GLM 4.7 AI model_

So, you know how everyone's benchmarking LLMs these days? Like, "oh look, this model scored 87.3% on this synthetic test suite" or whatever. Boring, right? I mean, who cares if a model can reverse a string perfectly when you'll never ask it to reverse a string in real life? üòõ

That's why I built **RaceBench** ‚Äî a benchmarking tool that actually tests something useful. How well different LLMs can write real, working JavaScript code. Not some "hello world" nonsense, but actual creative coding tasks ‚Äî games, visualisations, interactive stuff you might actually want to build.

## The idea (because every project needs an origin story)

One day I was curious: if I ask different AI models to write the same game, which one would actually produce playable code? Not "technically correct but crashes on the third frame" code, but stuff you could run and enjoy.

So I created a benchmark where I ask models to build a 2D scroll shooter game using Three.js. Not exactly "easy mode" for an AI ‚Äî it requires understanding 3D graphics, game loops, user input, and making everything work together without blowing up the browser. Spoiler: some models handle it beautifully, others... well, let's say it's educational to watch them try üòä

## What it actually does

RaceBench is a static HTML dashboard that displays results from testing various LLMs on JavaScript code generation. Here's the cool part:

- **Real-world benchmarking** ‚Äî we test creative coding tasks, not synthetic "reverse this string" nonsense
- **Executable results** ‚Äî every model's output can be run directly. See the code, click preview, judge for yourself
- **Multi-dimensional analysis** ‚Äî not just "did it work?" We track prompt tokens, completion tokens, TTFT (time to first token), generation time, and success rate
- **Educational value** ‚Äî compare how different models structure their code, their approaches to the same problem, and learn from the variety (even the bad ones!)

## How it works (the technical bits, minus the boring parts)

### The setup

Everything's static ‚Äî pure HTML/CSS/JS with no backend. Deploy it anywhere. GitHub Pages? Sure. Netlify? Why not. Your Raspberry Pi at home? Absolutely. The whole thing weighs about as much as a small hamster üêπ

```bash
git clone https://github.com/DimitriGilbert/racebench.git

# Deploy to GitHub Pages (or your jungle site of choice)
# I mean, whatever floats your boat
```

### The test case

The benchmark uses a 2D scroll shooter game built with Three.js as the test scenario. Each model gets the same prompt and has to produce working code. We extract the last ````runjs` code block from each response (because models love to chat before giving you the goods, you know?).

### What we track

For each model, RaceBench captures:

- **Prompt tokens** ‚Äî how much it needed to understand what we wanted
- **Completion tokens** ‚Äî how verbose it got with the solution
- **TTFT** (Time To First Token) ‚Äî responsiveness metrics
- **Generation time** ‚Äî total time to produce the code
- **Success rate** ‚Äî did it actually run? Or did it explode?

### The UI (because I care about aesthetics, surprisingly)

Nice dark mode UI that doesn't look like it was designed by a sysadmin in 2003. Two view modes:

- **Card grid** ‚Äî pretty cards for each model with key metrics at a glance
- **Sortable table** ‚Äî for when you want to data-nerd out and sort by TTFT or token count

Click any model and you get:
- Live preview (opens in a new tab, runs the actual code)
- Raw response view (see exactly what the model said)
- All the metrics broken down

## Versions (because software never ends)

### v1: The brave beginning

Tested 18 models. Learned a lot. Realised some models are impressively creative while others are... trying their best. Let's put it that way. üòõ

### v2: The refinement

Narrowed it down to 10 models. Better metrics extraction. Cleaner UI. Still watching models struggle with Three.js in entertaining ways.

## Tech stack (kept it simple, you're welcome)

- **Node.js** ‚Äî because JavaScript is life
- **Tailwind CSS (via CDN)** ‚Äî styling without the build step headaches
- **Simple-Datatables** ‚Äî for when you want sortable tables but don't want to write them yourself
- **Three.js (dynamic)** ‚Äî loaded on demand for the live previews

Minimal dependencies. No complex build pipelines. Just works. I like that philosophy.

## The models (participants in the chaos)

We've tested a bunch of the big names:

- Claude Sonnet 4
- DeepSeek R1
- GPT-4.1
- Gemini 2.5
- Grok 4
- Kimi K2
- Qwen3
- ...and more

Each one brings its own personality to the code. Some are terse and efficient. Others are verbose and over-engineered. A few produce code that makes you question everything you know about software development. All educational in their own way üòä

## See it in action

Don't just take my word for it ‚Äî go play with the actual dashboard:

**[Live Demo](https://dimitrigilbert.github.io/racebench/scroller/index.html)**

Click around. Run some code. See which model you'd want on your team. Spoiler: the answer might surprise you (or it might be exactly what you expect. LLMs are weird like that).

## What makes this different from other benchmarks?

I'm glad you asked (I know you didn't, but pretend you did) üòõ

1. **Real tasks, not toy problems** ‚Äî we test actual creative coding, not "sort this array" nonsense
2. **You can run the results** ‚Äî not just a score, actual working code you can inspect and execute
3. **No server needed** ‚Äî static HTML means you can host it anywhere or even run it locally
4. **Transparent metrics** ‚Äî see exactly what we measured and how
5. **Educational** ‚Äî compare approaches, learn patterns, see what works (and what definitely doesn't)

## What I learned (because building stuff teaches you things)

- Some models are surprisingly good at 3D graphics code
- Others clearly haven't seen many Three.js examples in their training
- TTFT doesn't always correlate with code quality
- Verbose models aren't necessarily better ‚Äî sometimes they're just... chatty
- The best model depends on what you value: speed, efficiency, code clarity, or just getting something that runs

## Future plans (the "if I get around to it" section)

I'd like to:

- Add more test cases (different types of creative coding challenges)
- Test more models (new ones keep popping up like mushrooms)
- Improve the metrics (maybe add code quality scoring?)
- Add a "leaderboard" view for those who love rankings
- Maybe, possibly, contribute the data to some broader benchmark effort (if such a thing exists and wants this kind of real-world data)

But let's be honest ‚Äî I'll probably add a new test case before I remember to do any of that. Priorities, right? üòä

## Wrapping up

RaceBench isn't trying to be the definitive AI benchmark. It's just me trying to answer a simple question: "Which AI writes the best JavaScript code for real projects?" And having fun seeing them all try.

If you find it useful, great. If you learn something from the different code styles, even better. If you just enjoy clicking through live previews of AI-generated games and thinking "wow, that actually works," then I've done my job.

Feel free to fork it, add your own models, test different prompts. The more data, the better. Just don't ask me which model is "best" ‚Äî that depends on what you're building, your timeline, your budget, and probably the phase of the moon. LLMs are like that üòõ

**GitHub:** [DimitriGilbert/racebench](https://github.com/DimitriGilbert/racebench)

Now go race some models. May the best code win.

{{% goodbye %}}
10:T23a9,
_Ce contenu a √©t√© g√©n√©r√© par le mod√®le AI GLM 4.7 et not-ai-writer_

Alors, vous voyez comme tout le monde benchmarke des LLMs en ce moment ? Genre, "oh regarde, ce mod√®le a score 87,3% sur cette suite de tests synth√©tique" ou quoi que ce soit dans le genre. Ennuyeux, non ? Je veux dire, on s'en fiche de savoir si un mod√®le peut inverser une cha√Æne parfaitement quand on ne lui demandera jamais d'inverser une cha√Æne dans la vraie vie ? üòõ

C'est pour √ßa que j'ai cr√©√© **RaceBench** ‚Äî un outil de benchmark qui teste vraiment quelque chose d'utile. √Ä quel point diff√©rents LLMs peuvent √©crire du code JavaScript r√©el qui fonctionne. Pas des trucs de "hello world" nuls, mais de vraies t√¢ches de code cr√©atif ‚Äî des jeux, des visualisations, des trucs interactifs que vous pourriez vraiment vouloir construire.

## L'id√©e (parce que tout projet a besoin d'une histoire d'origine)

Un jour j'√©tais curieux: si je demande √† diff√©rents mod√®les d'IA d'√©crire le m√™me jeu, lequel produirait vraiment du code jouable ? Pas du code "techniquement correct mais qui plante √† la troisi√®me frame", mais des trucs qu'on pourrait lancer et appr√©cier.

Donc j'ai cr√©√© un benchmark o√π je demande aux mod√®les de construire un jeu de tir scroll 2D en utilisant Three.js. Pas vraiment "mode facile" pour une IA ‚Äî √ßa demande de comprendre la 3D, les boucles de jeu, les entr√©es utilisateur, et de faire tout fonctionner ensemble sans faire exploser le navigateur. Spoiler: certains mod√®les g√®rent √ßa magnifiquement, d'autres... enfin, disons que c'est √©ducatif de les essayer üòä

## Ce que √ßa fait vraiment

RaceBench est un dashboard HTML statique qui affiche les r√©sultats de tests de divers LLMs sur la g√©n√©ration de code JavaScript. Voici la partie cool:

- **Benchmarking r√©el** ‚Äî on teste des t√¢ches de code cr√©atif, pas des trucs synth√©tiques du genre "inverse cette cha√Æne"
- **R√©sultats ex√©cutables** ‚Äî la sortie de chaque mod√®le peut √™tre lanc√©e directement. Voyez le code, cliquez sur preview, jugez par vous-m√™me
- **Analyse multi-dimensionnelle** ‚Äî pas juste "√ßa a march√© ?" On tracke les tokens de prompt, les tokens de compl√©tion, le TTFT (temps jusqu'au premier token), le temps de g√©n√©ration, et le taux de succ√®s
- **Valeur √©ducative** ‚Äî comparez comment diff√©rents mod√®les structurent leur code, leurs approches au m√™me probl√®me, et apprenez de la vari√©t√© (m√™me des mauvais !)

## Comment √ßa marche (les bits techniques, moins les parties ennuyeuses)

### Le setup

Tout est statique ‚Äî du HTML/CSS/JS pur sans backend. D√©ployez-le n'importe o√π. GitHub Pages ? S√ªr. Netlify ? Pourquoi pas. Votre Raspberry Pi √† la maison ? Absolument. Le tout p√®se √† peu pr√®s le poids d'un petit hamster üêπ

```bash
git clone https://github.com/DimitriGilbert/racebench.git

# Deploy to GitHub Pages (or your jungle site of choice)
# I mean, whatever floats your boat
```

### Le cas de test

Le benchmark utilise un jeu de tir scroll 2D construit avec Three.js comme sc√©nario de test. Chaque mod√®le re√ßoit le m√™me prompt et doit produire du code qui fonctionne. On extrait le dernier bloc de code ```runjs de chaque r√©ponse (parce que les mod√®les adorent discuter avant de vous donner la marchandise, vous voyez ?).

### Ce qu'on tracke

Pour chaque mod√®le, RaceBench capture:

- **Prompt tokens** ‚Äî combien il en a eu besoin pour comprendre ce qu'on voulait
- **Completion tokens** ‚Äî √† quel point il a √©t√© verbeux avec la solution
- **TTFT** (Time To First Token) ‚Äî m√©triques de r√©activit√©
- **Generation time** ‚Äî temps total pour produire le code
- **Success rate** ‚Äî √ßa a vraiment tourn√© ? Ou √ßa a explos√© ?

### L'UI (parce que je me soucie de l'esth√©tique, √©tonnamment)

Une superbe interface dark mode qui n'a pas l'air d'avoir √©t√© con√ßue par un admin sys en 2003. Deux modes de vue:

- **Grille de cartes** ‚Äî de belles cartes pour chaque mod√®le avec les m√©triques cl√©s en un coup d'≈ìil
- **Tableau triable** ‚Äî pour quand vous voulez faire le data-nerd et trier par TTFT ou nombre de tokens

Cliquez sur n'importe quel mod√®le et vous obtenez:
- Live preview (s'ouvre dans un nouvel onglet, lance le vrai code)
- Vue de la r√©ponse brute (voyez exactement ce que le mod√®le a dit)
- Toutes les m√©triques d√©taill√©es

## Versions (parce que le logiciel ne se termine jamais)

### v1: Le d√©but courageux

Test√© 18 mod√®les. Appris beaucoup. R√©alis√© que certains mod√®les sont impressionnamment cr√©atifs tandis que d'autres sont... essaient leur meilleur. Mettons les choses comme √ßa. üòõ

### v2: Le raffinement

R√©duit √† 10 mod√®les. Meilleure extraction des m√©triques. UI plus propre. Toujours √† regarder les mod√®les gal√©rer avec Three.js de mani√®re amusante.

## Stack technique (gard√© simple, de rien)

- **Node.js** ‚Äî parce que JavaScript est la vie
- **Tailwind CSS (via CDN)** ‚Äî du style sans les maux de t√™te de build
- **Simple-Datatables** ‚Äî pour quand vous voulez des tableaux triables mais ne voulez pas les √©crire vous-m√™me
- **Three.js (dynamique)** ‚Äî charg√© √† la demande pour les previews en direct

D√©pendances minimales. Pas de pipelines de build complexes. Juste √ßa marche. J'aime cette philosophie.

## Les mod√®les (participants dans le chaos)

On a test√© plein de grands noms:

- Claude Sonnet 4
- DeepSeek R1
- GPT-4.1
- Gemini 2.5
- Grok 4
- Kimi K2
- Qwen3
- ...et plus

Chacun apporte sa propre personnalit√© au code. Certains sont laconiques et efficaces. D'autres sont verbeux et sur-ing√©nier√©s. Quelques-uns produisent du code qui vous fait remettre en question tout ce que vous savez sur le d√©veloppement logiciel. Tous √©ducatifs √† leur mani√®re üòä

## Voyez-le en action

Ne me croyez pas sur parole ‚Äî allez jouer avec le vrai dashboard:

**[D√©mo Live](https://dimitrigilbert.github.io/racebench/scroller/index.html)**

Cliquez autour. Lancez du code. Voyez quel mod√®le vous voudriez dans votre √©quipe. Spoiler: la r√©ponse pourrait vous surprendre (ou elle pourrait √™tre exactement ce que vous attendez. Les LLMs sont bizarres comme √ßa).

## Ce qui rend √ßa diff√©rent des autres benchmarks ?

Je suis ravi que vous demandiez (je sais que vous n'avez pas demand√©, mais faites semblant) üòõ

1. **Vraies t√¢ches, pas probl√®mes jouets** ‚Äî on teste du code cr√©atif r√©el, pas des trucs du genre "trie ce tableau"
2. **Vous pouvez lancer les r√©sultats** ‚Äî pas juste un score, du vrai code qui fonctionne que vous pouvez inspecter et ex√©cuter
3. **Pas de serveur n√©cessaire** ‚Äî HTML statique signifie que vous pouvez l'h√©berger n'importe o√π ou m√™me le lancer localement
4. **M√©triques transparentes** ‚Äî voyez exactement ce qu'on a mesur√© et comment
5. **√âducatif** ‚Äî comparez les approches, apprenez des patterns, voyez ce qui marche (et ce qui ne marche DEFINITIVEMENT pas)

## Ce que j'ai appris (parce que construire des trucs vous apprend des choses)

- Certains mod√®les sont √©tonnamment bons en code graphique 3D
- D'autres n'ont clairement pas vu beaucoup d'exemples Three.js dans leur entra√Ænement
- Le TTFT ne correlate pas toujours avec la qualit√© du code
- Les mod√®les verbeux ne sont pas n√©cessairement meilleurs ‚Äî parfois ils sont juste... bavards
- Le meilleur mod√®le d√©pend de ce que vous valorisez: vitesse, efficacit√©, clart√© du code, ou juste avoir quelque chose qui tourne

## Plans futurs (la section "si je m'y retrouve")

J'aimerais:

- Ajouter plus de cas de test (diff√©rents types de d√©fis de code cr√©atif)
- Tester plus de mod√®les (de nouveaux surgissent comme des champignons)
- Am√©liorer les m√©triques (peut-√™tre ajouter un scoring de qualit√© du code ?)
- Ajouter une vue "leaderboard" pour ceux qui adorent les classements
- Peut-√™tre, possiblement, contribuer les donn√©es √† un effort de benchmark plus large (si un tel truc existe et veut ce genre de donn√©es du monde r√©el)

Mais soyons honn√™tes ‚Äî j'ajouterai probablement un nouveau cas de test avant de me souvenir de faire quoi que ce soit de tout √ßa. Les priorit√©s, quoi ? üòä

## Conclusion

RaceBench n'essaie pas d'√™tre le benchmark AI d√©finitif. C'est juste moi qui essaie de r√©pondre √† une question simple: "Quelle IA √©crit le meilleur code JavaScript pour des projets r√©els ?" Et m'amuse √† les voir tous essayer.

Si vous le trouvez utile, super. Si vous apprenez quelque chose des diff√©rents styles de code, encore mieux. Si vous aimez juste cliquer √† travers des previews en direct de jeux g√©n√©r√©s par IA et penser "wow, √ßa marche vraiment," alors j'ai fait mon boulot.

N'h√©sitez pas √† le forker, ajouter vos propres mod√®les, tester diff√©rents prompts. Plus il y a de donn√©es, mieux c'est. Ne me demandez pas quel mod√®le est "le meilleur" ‚Äî √ßa d√©pend de ce que vous construisez, votre timeline, votre budget, et probablement la phase de la lune. Les LLMs sont comme √ßa üòõ

**GitHub:** [DimitriGilbert/racebench](https://github.com/DimitriGilbert/racebench)

Maintenant allez faire la course √† quelques mod√®les. Que le meilleur code gagne.

{{% goodbye %}}
6:["$","$Le",null,{"tag":"llm","posts":[{"slug":"projects-racebench","title":"RaceBench","description":"Racing LLMs against each other to see who writes the best JavaScript. Spoiler: it's chaos out there!","date":"$D2025-01-17T09:00:00.000Z","category":"General","tags":["benchmark","llm","ai","testing"],"content":"$f","readTime":7,"toc":[{"level":2,"title":"The idea (because every project needs an origin story)","id":"the-idea-because-every-project-needs-an-origin-story"},{"level":2,"title":"What it actually does","id":"what-it-actually-does"},{"level":2,"title":"How it works (the technical bits, minus the boring parts)","id":"how-it-works-the-technical-bits-minus-the-boring-parts"},{"level":3,"title":"The setup","id":"the-setup"},{"level":1,"title":"Deploy to GitHub Pages (or your jungle site of choice)","id":"deploy-to-github-pages-or-your-jungle-site-of-choice"},{"level":1,"title":"I mean, whatever floats your boat","id":"i-mean-whatever-floats-your-boat"},{"level":3,"title":"The test case","id":"the-test-case"},{"level":3,"title":"What we track","id":"what-we-track"},{"level":3,"title":"The UI (because I care about aesthetics, surprisingly)","id":"the-ui-because-i-care-about-aesthetics-surprisingly"},{"level":2,"title":"Versions (because software never ends)","id":"versions-because-software-never-ends"},{"level":3,"title":"v1: The brave beginning","id":"v1-the-brave-beginning"},{"level":3,"title":"v2: The refinement","id":"v2-the-refinement"},{"level":2,"title":"Tech stack (kept it simple, you're welcome)","id":"tech-stack-kept-it-simple-youre-welcome"},{"level":2,"title":"The models (participants in the chaos)","id":"the-models-participants-in-the-chaos"},{"level":2,"title":"See it in action","id":"see-it-in-action"},{"level":2,"title":"What makes this different from other benchmarks?","id":"what-makes-this-different-from-other-benchmarks"},{"level":2,"title":"What I learned (because building stuff teaches you things)","id":"what-i-learned-because-building-stuff-teaches-you-things"},{"level":2,"title":"Future plans (the \"if I get around to it\" section)","id":"future-plans-the-if-i-get-around-to-it-section"},{"level":2,"title":"Wrapping up","id":"wrapping-up"}],"directory":"Projects/RaceBench","relativePath":"Projects/RaceBench/index.md","filePath":"/home/didi/workspace/Code/dbuild.io/apps/web/content/blog/Projects/RaceBench/index.md","isCategory":false},{"slug":"projects-racebench-fr","title":"RaceBench fr","description":"Faire s'affronter des LLMs pour voir qui √©crit le meilleur JavaScript. Spoiler: c'est le chaos complet !","date":"$D2025-01-17T09:00:00.000Z","category":"General","tags":["benchmark","llm","ai","testing","french"],"content":"$10","readTime":8,"toc":[{"level":2,"title":"L'id√©e (parce que tout projet a besoin d'une histoire d'origine)","id":"lide-parce-que-tout-projet-a-besoin-dune-histoire-dorigine"},{"level":2,"title":"Ce que √ßa fait vraiment","id":"ce-que-a-fait-vraiment"},{"level":2,"title":"Comment √ßa marche (les bits techniques, moins les parties ennuyeuses)","id":"comment-a-marche-les-bits-techniques-moins-les-parties-ennuyeuses"},{"level":3,"title":"Le setup","id":"le-setup"},{"level":1,"title":"Deploy to GitHub Pages (or your jungle site of choice)","id":"deploy-to-github-pages-or-your-jungle-site-of-choice"},{"level":1,"title":"I mean, whatever floats your boat","id":"i-mean-whatever-floats-your-boat"},{"level":3,"title":"Le cas de test","id":"le-cas-de-test"},{"level":3,"title":"Ce qu'on tracke","id":"ce-quon-tracke"},{"level":3,"title":"L'UI (parce que je me soucie de l'esth√©tique, √©tonnamment)","id":"lui-parce-que-je-me-soucie-de-lesthtique-tonnamment"},{"level":2,"title":"Versions (parce que le logiciel ne se termine jamais)","id":"versions-parce-que-le-logiciel-ne-se-termine-jamais"},{"level":3,"title":"v1: Le d√©but courageux","id":"v1-le-dbut-courageux"},{"level":3,"title":"v2: Le raffinement","id":"v2-le-raffinement"},{"level":2,"title":"Stack technique (gard√© simple, de rien)","id":"stack-technique-gard-simple-de-rien"},{"level":2,"title":"Les mod√®les (participants dans le chaos)","id":"les-modles-participants-dans-le-chaos"},{"level":2,"title":"Voyez-le en action","id":"voyez-le-en-action"},{"level":2,"title":"Ce qui rend √ßa diff√©rent des autres benchmarks ?","id":"ce-qui-rend-a-diffrent-des-autres-benchmarks"},{"level":2,"title":"Ce que j'ai appris (parce que construire des trucs vous apprend des choses)","id":"ce-que-jai-appris-parce-que-construire-des-trucs-vous-apprend-des-choses"},{"level":2,"title":"Plans futurs (la section \"si je m'y retrouve\")","id":"plans-futurs-la-section-si-je-my-retrouve"},{"level":2,"title":"Conclusion","id":"conclusion"}],"directory":"Projects/RaceBench-fr","relativePath":"Projects/RaceBench-fr/index.md","filePath":"/home/didi/workspace/Code/dbuild.io/apps/web/content/blog/Projects/RaceBench-fr/index.md","isCategory":false}]}]
c:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
a:[["$","meta","0",{"charSet":"utf-8"}],["$","title","1",{"children":"Dbuild.dev"}],["$","meta","2",{"name":"description","content":"Dbuild.dev is a portfolio and blog showcasing projects and insights"}],["$","link","3",{"rel":"manifest","href":"/manifest.webmanifest","crossOrigin":"$undefined"}],["$","meta","4",{"name":"robots","content":"index, follow"}],["$","meta","5",{"name":"googlebot","content":"index, follow"}],["$","meta","6",{"property":"og:title","content":"Dbuild.dev"}],["$","meta","7",{"property":"og:description","content":"Dbuild.dev is a portfolio and blog showcasing projects and insights"}],["$","meta","8",{"property":"og:url","content":"https://dbuild.dev/"}],["$","meta","9",{"property":"og:site_name","content":"Dbuild.dev"}],["$","meta","10",{"property":"og:locale","content":"en_US"}],["$","meta","11",{"property":"og:image","content":"https://dbuild.dev/og-image.jpg"}],["$","meta","12",{"property":"og:image:width","content":"1200"}],["$","meta","13",{"property":"og:image:height","content":"630"}],["$","meta","14",{"property":"og:image:alt","content":"Dbuild.dev - Portfolio and Blog"}],["$","meta","15",{"property":"og:type","content":"website"}],["$","meta","16",{"name":"twitter:card","content":"summary_large_image"}],["$","meta","17",{"name":"twitter:title","content":"Dbuild.dev"}],["$","meta","18",{"name":"twitter:description","content":"Dbuild.dev is a portfolio and blog showcasing projects and insights"}],["$","meta","19",{"name":"twitter:image","content":"https://dbuild.dev/og-image.jpg"}],["$","link","20",{"rel":"icon","href":"/favicon.ico","type":"image/x-icon","sizes":"32x32"}]]
8:null
